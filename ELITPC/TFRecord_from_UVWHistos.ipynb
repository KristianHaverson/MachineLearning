{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/04\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import ROOT\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################################################\n",
    "def _array_float32_feature(ndarray):\n",
    "    return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array.reshape(-1)))\n",
    "################################################\n",
    "##FIXME: reduce precision to 32 bits\n",
    "################################################\n",
    "def _array_int64_feature(ndarray):\n",
    "    return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array.reshape(-1)))\n",
    "################################################\n",
    "################################################\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "################################################\n",
    "################################################\n",
    "def create_features(x, y):\n",
    "    dtype_feature_x = _array_float32_feature(x)\n",
    "    dtype_feature_y = _array_int64_feature(y)\n",
    "    d_feature = {}\n",
    "    d_feature['UVW_data'] = dtype_feature_x(x)\n",
    "    d_feature['label'] = dtype_feature_y(y)\n",
    "    features = tf.train.Features(feature = d_feature)\n",
    "    return features\n",
    "################################################\n",
    "################################################\n",
    "def createWriter(fileName):\n",
    "    result_tf_file = fileName + '.tfrecords'\n",
    "    writer = tf.io.TFRecordWriter(result_tf_file)\n",
    "    return writer, result_tf_file\n",
    "################################################\n",
    "################################################\n",
    "def saveSingleExampleToTFRecord(writer, features):\n",
    "    example = tf.train.Example(features = features)\n",
    "    serialized = example.SerializeToString()\n",
    "    writer.write(serialized)\n",
    "################################################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods reading the ROOT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################################################\n",
    "def get_numpy_from_histo(root_histo):\n",
    "    nBinsX = root_histo.GetNbinsX()\n",
    "    nBinsY = root_histo.GetNbinsY()\n",
    "    nBinsY = 92 ##FIXME: Need uniform frame size\n",
    "    numpy_histo = np.zeros((nBinsX,nBinsY))\n",
    "    for iBinX in range(0, nBinsX):\n",
    "        for iBinY in range(0, nBinsY):\n",
    "            numpy_histo[iBinX, iBinY] = root_histo.GetBinContent(iBinX, iBinY)\n",
    "    return numpy_histo\n",
    "################################################\n",
    "##FIX ME: provide source of labels\n",
    "################################################\n",
    "def read_root(fileName, normalize=False):\n",
    "    projection_histos = {\"U\":0, \"V\":0, \"W\":0}\n",
    "    rootFile = ROOT.TFile(fileName,\"r\")\n",
    "    keysList = rootFile.GetListOfKeys()\n",
    "    numberOfHistos = len(keysList)\n",
    "    label = 0\n",
    "    index = 0\n",
    "    while index<numberOfHistos:\n",
    "        objName = keysList[index].GetName()\n",
    "        eventId = objName.split(\"evt\")[1]\n",
    "        for projName in projection_histos.keys():\n",
    "            histo_name = \"hraw_\"+projName+\"_vs_time_evt\"+eventId\n",
    "            root_histo = rootFile.Get(histo_name)\n",
    "            numpy_histo = get_numpy_from_histo(root_histo)\n",
    "            if normalize: \n",
    "                maxValue = np.amax(numpy_histo)\n",
    "                numpy_histo = np.where(numpy_histo<0,0,numpy_histo/maxValue)\n",
    "            #print(\"histo name: \",histo_name,\" shape:\",numpy_histo.shape)\n",
    "            projection_histos[projName] = numpy_histo\n",
    "       \n",
    "        features = np.stack(arrays=list(projection_histos.values()), axis=2)\n",
    "        features_cropped = np.stack(arrays=list(projection_histos.values()), axis=2)\n",
    "        labels = np.array([0,0])\n",
    "        if index==0:\n",
    "            print(\"features.shape: \",features.shape)\n",
    "            print(\"labels.shape: \",labels.shape)\n",
    "        index += 3 \n",
    "        #print(\"eventId:\",eventId)\n",
    "        yield features, labels \n",
    "################################################\n",
    "##Test histogram reading\n",
    "################################################\n",
    "fileName = \"/scratch/akalinow/ProgrammingProjects/MachineLearning/ELITPC/data/UVWProjections_2018-06-19T15:13:33.941_0008.root\"\n",
    "fileName = \"/scratch/akalinow/ProgrammingProjects/MachineLearning/ELITPC/data/ProjectionHistos_2021-09-10T12:08:25.751_0.root\"\n",
    "\n",
    "f = ROOT.TFile(fileName)\n",
    "h_U = f.Get(\"hraw_U_vs_time_evt0\")\n",
    "c1 = ROOT.TCanvas()\n",
    "c1.Draw()\n",
    "h_U.Draw(\"col\")\n",
    "\n",
    "for item in read_root(fileName):\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final conversion methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n",
      "features.shape:  (512, 92, 3)\n",
      "labels.shape:  (2,)\n",
      "features.shape:  (512, 92, 3)\n",
      "labels.shape:  (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d0433a41683a>:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "  numpy_histo = np.where(numpy_histo<0,0,numpy_histo/maxValue)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape:  (512, 92, 3)\n",
      "labels.shape:  (2,)\n",
      "features.shape:  (512, 92, 3)\n",
      "labels.shape:  (2,)\n",
      "features.shape:  (512, 92, 3)\n",
      "labels.shape:  (2,)\n",
      "Serializing 4870 examples into /scratch/akalinow/ProgrammingProjects/MachineLearning/ELITPC/data/ProjectionHistos_2021-09-10T12:08:25.751_0.root.tfrecords done!\n",
      "Execution time: 507.73 seconds\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "################################################\n",
    "def ROOT_to_TFRecord(normalize=False):\n",
    "    path = \"/scratch/akalinow/ProgrammingProjects/MachineLearning/ELITPC/data/\"\n",
    "    file_list = glob.glob(path + \"ProjectionHistos_2021-09-10*.root\")\n",
    "    number_of_files = len(file_list)\n",
    "    number_of_examples = 0\n",
    "    print (\"Found {} files\".format(number_of_files))\n",
    "    start_time = time.perf_counter()\n",
    "    writer, result_tf_file = createWriter(fileName)\n",
    "    for idx, file in enumerate(file_list):\n",
    "        for numpy_histogram, labels in read_root(file, normalize=normalize):\n",
    "            features = create_features(numpy_histogram, labels)\n",
    "            saveSingleExampleToTFRecord(writer, features)\n",
    "            number_of_examples+=1\n",
    "        if idx == number_of_files - 1:\n",
    "            writer.close()\n",
    "            print (\"Serializing {} examples into {} done!\".format(number_of_examples,result_tf_file))\n",
    "            print(\"Execution time: {:.2f} seconds\".format(time.perf_counter() - start_time))\n",
    "################################################\n",
    "################################################\n",
    "ROOT_to_TFRecord(normalize=False)\n",
    "################################################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UVW_data': <tf.Tensor: shape=(512, 92, 3), dtype=float32, numpy=\n",
      "array([[[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]], dtype=float32)>, 'label': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 11:34:13.077142: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-26 11:34:13.077166: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-26 11:34:13.077187: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2021-10-26 11:34:13.077856: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-26 11:34:13.138032: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "featuresShape = (512, 92, 3)\n",
    "cropped_featuresShape = (64, 64, 3)\n",
    "labelsShape = (2,)\n",
    "################################################\n",
    "################################################\n",
    "feature_description = {\n",
    "    'UVW_data': tf.io.FixedLenFeature(featuresShape, tf.float32),\n",
    "    'label': tf.io.FixedLenFeature(labelsShape, tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "################################################\n",
    "################################################\n",
    "def readTFRecordFile(fileNames):\n",
    "    raw_dataset = tf.data.TFRecordDataset(fileNames)\n",
    "    return raw_dataset.map(_parse_function)\n",
    "################################################\n",
    "################################################\n",
    "#fileNames = [\"UVWProjections_2018-06-19T15:13:33.941_0008.tfrecords\"]\n",
    "fileNames = [\"ProjectionHistos_2021-09-10T12:08:25.751.root.tfrecords\"]\n",
    "dataset = readTFRecordFile(fileNames)\n",
    "\n",
    "for item in dataset.take(1):\n",
    "      print(repr(item))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
