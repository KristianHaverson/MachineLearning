{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 14:08:02.828531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "import uproot\n",
    "import awkward as ak\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.data import Dataset, TFRecordDataset\n",
    "from tensorflow.data.experimental import TFRecordWriter\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Features, Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 14:13:23.960551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 24.7 s, total: 2min 2s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import io_functions as io\n",
    "importlib.reload(io)\n",
    "\n",
    "train_files = ['/scratch_ssd/akalinow/ELITPC/data/E_11_sigma_2/out_C_arr_1.root:TPCData']\n",
    "batchSize = 200\n",
    "\n",
    "datasetGenerator = partial(io.generator, files=train_files, batchSize=batchSize)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "     datasetGenerator,\n",
    "     output_signature=(\n",
    "         tf.TensorSpec(shape=(batchSize,)+ io.projections.shape, dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(batchSize, 9), dtype=tf.float32)))\n",
    "\n",
    "\n",
    "for aBatch in train_dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 24s, sys: 663 ms, total: 2min 25s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataDirectory = \"/scratch_hdd/akalinow/ELITPC/PythonAnalysis/data/E_11_sigma_2/\"\n",
    "dataFile = \"out_C_arr_1.root\"\n",
    "treeName = \":TPCData\"\n",
    "inputObj = dataDirectory+dataFile+treeName\n",
    "batchSize = 1\n",
    "\n",
    "fields = [\n",
    "    #\"SimEvent/reactionType\",\n",
    "    \"SimEvent/tracks/tracks.startPos\",\n",
    "    \"SimEvent/tracks/tracks.stopPos\",\n",
    "    #\"SimEvent/tracks/tracks.prim.pID\",\n",
    "    #\"SimEvent/tracks/tracks.prim.fourMomentum\",\n",
    "    #\"Event/myChargeMap\",\n",
    "    \"Event/myChargeArray*\",\n",
    "    \"SimEvent/tracks/tracks.truncatedStartPosUVWT.*\",\n",
    "    \"SimEvent/tracks/tracks.truncatedStopPosUVWT.*\",\n",
    "]\n",
    "\n",
    "\n",
    "def generator(files):\n",
    "    for array in uproot.iterate(files, step_size=batchSize, filter_name=fields, library=\"ak\"):\n",
    "      \n",
    "        fX = array['tracks.startPos']['fX'].to_numpy()\n",
    "        fY = array['tracks.startPos']['fY'].to_numpy()\n",
    "        fZ = array['tracks.startPos']['fZ'].to_numpy()\n",
    "        startPos = np.stack([fX, fY, fZ], axis=1)[:,:,[0]]\n",
    "        \n",
    "        fX = array['tracks.stopPos']['fX'].to_numpy()\n",
    "        fY = array['tracks.stopPos']['fY'].to_numpy()\n",
    "        fZ = array['tracks.stopPos']['fZ'].to_numpy()\n",
    "        stopPos = np.stack([fX, fY, fZ], axis=1)\n",
    "        \n",
    "        target = np.concatenate([startPos, stopPos], axis=2)\n",
    "        \n",
    "        features = array[\"myChargeArray[3][3][256][512]\"].to_numpy()\n",
    "        features = np.sum(features, axis=2)\n",
    "        features = np.moveaxis(features, 1, -1)\n",
    "\n",
    "        yield features, target\n",
    "        \n",
    "for item in generator(files=inputObj):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 10.3 s, total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import io_functions as io\n",
    "importlib.reload(io)\n",
    "\n",
    "for item in io.generator(files=inputObj):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveDatasetToTFRecord(dataset, fileName):  \n",
    "    dataset = dataset.map(.io.serialize_tensor)\n",
    "    writer = tf.data.experimental.TFRecordWriter(fileName, compression_type=\"GZIP\")\n",
    "    writer.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tf = 'startPos.tfrecord'\n",
    "item_of_TPCData_list = 'SimEvent/tracks/tracks.startPos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tf = 'stopPos.tfrecord'\n",
    "item_of_TPCData_list = 'SimEvent/tracks/tracks.stopPos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord(path_tf, item_of_TPCData_list):\n",
    "    with tf.io.TFRecordWriter(path_tf) as file_writer:\n",
    "        for x in TPCData.iterate(item_of_TPCData_list, step_size=1):\n",
    "\n",
    "            record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"fX\": tf.train.Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(x[item_of_TPCData_list]['fX']).numpy()])),\n",
    "                \"fY\": tf.train.Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(x[item_of_TPCData_list]['fY']).numpy()])),\n",
    "                \"fZ\": tf.train.Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(x[item_of_TPCData_list]['fZ']).numpy()])),\n",
    "\n",
    "\n",
    "            })).SerializeToString()\n",
    "            file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tfrecord(path_tf, item_of_TPCData_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tfrecord_files = ['stopPos.tfrecord', 'startPos.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(list_of_tfrecord_files)\n",
    "\n",
    "filename = 'root.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_generator(\n",
    "#     datasetGenerator,\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(io.projections.shape), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(9), dtype=tf.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nStrips=256\n",
    "nTimeSlices = 512\n",
    "nProj = 3\n",
    "projections = np.zeros((nStrips,nTimeSlices, nProj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord_tfDataset(path_tf):\n",
    "    with tf.io.TFRecordWriter(path_tf) as file_writer:\n",
    "        for x in train_dataset:\n",
    "\n",
    "            record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"projections\": tf.train.Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(x).numpy()]),\n",
    "                                         )\n",
    "\n",
    "\n",
    "            })).SerializeToString()\n",
    "            file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_tfrecord_tfDataset('projections_test.tfrecord')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
