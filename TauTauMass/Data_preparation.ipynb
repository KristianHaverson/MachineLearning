{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/04\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from root_numpy import root2array\n",
    "from root_pandas import read_root\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "workerEnv = str(os.getenv(\"USER\"))\n",
    "workOnPrometheus = workerEnv.find(\"plg\")>-1\n",
    "inputDataPrefix = \"/home/user1/scratch/akalinow/\"\n",
    "if workOnPrometheus:\n",
    "    inputDataPrefix = \"/net/people/plgakalinow/plggcmsml/\"\n",
    "dataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/TauTauMass/data/25_01_2021/\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform pickled data to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"/home/user1/scratch/akalinow/ProgrammingProjects/RootAnalysis/build_Docker/RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.root\"\n",
    "\n",
    "for iChunk, dfChunk in enumerate(read_root(paths=fileName, key=\"Summary/tree\",chunksize=int(1000))):\n",
    "            print(\"\\tProcessing chunk: {}\".format(iChunk))\n",
    "            print(dfChunk)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumpyMatricesFromRawData(filePath):\n",
    "\n",
    "        legs, jets, global_params, properties = pd.read_pickle(filePath)\n",
    "        properties = OrderedDict(sorted(properties.items(), key=lambda t: t[0]))\n",
    "\n",
    "        print(\"no of legs: \", len(legs))\n",
    "        print(\"no of jets: \", len(jets))\n",
    "        print(\"global params: \", global_params.keys())\n",
    "        print(\"object properties:\",properties.keys())\n",
    "\n",
    "        genMass = np.array(global_params[\"genMass\"])\n",
    "        fastMTT = np.array(global_params[\"fastMTTMass\"])\n",
    "        visMass = np.array(global_params[\"visMass\"])\n",
    "        caMass = np.array(global_params[\"caMass\"])\n",
    "        \n",
    "        covMET00 = np.array(global_params[\"covMET00\"])\n",
    "        covMET01 = np.array(global_params[\"covMET01\"])\n",
    "        covMET10 = np.array(global_params[\"covMET10\"])\n",
    "        covMET11 = np.array(global_params[\"covMET11\"])\n",
    "    \n",
    "        leg1P4 = np.array(legs[0])\n",
    "        leg2P4 = np.array(legs[1])\n",
    "        leg1GenP4 = np.array(legs[2])\n",
    "        leg2GenP4 = np.array(legs[3])        \n",
    "        leg2Properties = np.array(properties[\"leg_2_decayMode\"])\n",
    "        leg1Properties = np.array(properties[\"leg_1_combreliso\"])\n",
    "        jet1P4 = np.array(jets[1])\n",
    "        jet2P4 = np.array(jets[2])        \n",
    "        met = np.array(jets[0][0:3])\n",
    "\n",
    "        genMass = np.reshape(genMass, (-1,1))\n",
    "        visMass = np.reshape(visMass, (-1,1))\n",
    "        caMass = np.reshape(caMass, (-1,1))\n",
    "        fastMTT = np.reshape(fastMTT, (-1,1))\n",
    "    \n",
    "        covMET00 = np.reshape(covMET00,(-1,1))\n",
    "        covMET01 = np.reshape(covMET01,(-1,1))\n",
    "        covMET10 = np.reshape(covMET10,(-1,1))\n",
    "        covMET11 = np.reshape(covMET11,(-1,1))\n",
    "        \n",
    "        leg2Properties = np.reshape(leg2Properties, (-1,1))\n",
    "        leg1Properties = np.reshape(leg1Properties, (-1,1))\n",
    "        leg1P4 = np.transpose(leg1P4)\n",
    "        leg2P4 = np.transpose(leg2P4)\n",
    "        leg1GenP4 = np.transpose(leg1GenP4)\n",
    "        leg2GenP4 = np.transpose(leg2GenP4)        \n",
    "        jet1P4 = np.transpose(jet1P4)\n",
    "        jet2P4 = np.transpose(jet2P4)\n",
    "        met = np.transpose(met)\n",
    "                       \n",
    "        columns = [\"genMass\", \"visMass\", \"caMass\", \"fastMTT\",\n",
    "                   \"covMET00\", \"covMET01\", \"covMET10\", \"covMET11\",\n",
    "                   \"leg1_e\", \"leg1_px\",\"leg1_py\",\"leg1_pz\", \n",
    "                   \"leg2_e\", \"leg2_px\",\"leg2_py\",\"leg2_pz\",\n",
    "                   \"leg_2_decayMode\",\n",
    "                   \"met\", \"met_x\", \"met_y\"]\n",
    "        \n",
    "        features = np.hstack((genMass, visMass, caMass, fastMTT, covMET00, covMET01, covMET10, covMET11, leg1P4, leg2P4, leg2Properties, met))\n",
    "        df = pd.DataFrame(data=features, columns=columns)     \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/user1/scratch/akalinow//ProgrammingProjects/MachineLearning/TauTauMass/data/25_01_2021//home/user1/scratch/akalinow//ProgrammingProjects/MachineLearning/TauTauMass/data/25_01_2021/RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.root\n"
     ]
    }
   ],
   "source": [
    "dataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/TauTauMass/data/25_01_2021/\"\n",
    "fileNames = glob.glob(dataDir + 'RootAnalysis_SVfitMLAnalysisMuTau*.root')\n",
    "\n",
    "#fileNames = [\"/home/user1/scratch/akalinow/ProgrammingProjects/RootAnalysis/build/RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.root\",\n",
    "#             \"/home/user1/scratch/akalinow/ProgrammingProjects/RootAnalysis/build/RootAnalysis_SVfitMLAnalysisMuTau_Pythia8_smearMET.root\",\n",
    "#             \"/home/user1/scratch/akalinow/ProgrammingProjects/RootAnalysis/build/RootAnalysis_SVfitMLAnalysisMuTau_DY_ggH125.root\"\n",
    "#            ]\n",
    " \n",
    "for fileName in fileNames:\n",
    "    print(\"Processing file:\",dataDir+fileName)\n",
    "    label = fileName.split(\"/\")[-1]\n",
    "    label = label.rstrip(\".pkl\")\n",
    "    label = label.rstrip(\".root\")\n",
    "    path = str(pathlib.Path(fileName).parent)+\"/\"\n",
    "    parquetFile = path+'df.parquet_{}.gzip'.format(label)\n",
    "    #df = getNumpyMatricesFromRawData(fileName)\n",
    "    df = read_root(paths=fileName, key=\"Summary/tree\")\n",
    "    df = df.sample(frac=1.0)\n",
    "    df.to_parquet(parquetFile, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data into TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((38,), ()), types: (tf.float64, tf.float32)>\n",
      "1080323    287.847656\n",
      "1617005    159.656158\n",
      "833214     237.852951\n",
      "1663734    167.583023\n",
      "243256      94.823570\n",
      "              ...    \n",
      "2404742     80.385017\n",
      "2890340    186.847946\n",
      "1657987    166.550674\n",
      "3073100    233.095627\n",
      "2613613    122.759987\n",
      "Name: genMass, Length: 3404302, dtype: float32\n",
      "(<tf.Tensor: shape=(38,), dtype=float64, numpy=\n",
      "array([ 1.00000000e+00,  0.00000000e+00,  1.77789902e+02,  2.89348358e+02,\n",
      "        2.86383240e+02,  9.99999978e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        9.99999978e-03,  1.26785290e+02,  4.01327466e+01, -1.79280704e+01,\n",
      "       -1.18922015e+02,  6.59298203e+01, -4.21450848e+01, -9.42902597e+00,\n",
      "        4.98032481e+01,  1.26785290e+02,  4.01327466e+01, -1.79280704e+01,\n",
      "       -1.18922015e+02,  6.59298203e+01, -4.21450848e+01, -9.42902597e+00,\n",
      "        4.98032481e+01,  4.26391736e+01,  3.60728146e+01, -2.27343611e+01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.00000000e+00])>, <tf.Tensor: shape=(), dtype=float32, numpy=287.84766>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-25 09:58:13.725772: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-01-25 09:58:13.725787: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-01-25 09:58:13.725802: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2021-01-25 09:58:13.726260: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-01-25 09:58:13.752642: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\n",
      "2021-01-25 09:58:13.753297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d2e1d60490 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-25 09:58:13.753309: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-01-25 09:58:14.599369: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1034907808 exceeds 10% of system memory.\n",
      "2021-01-25 09:58:15.351947: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1034907808 exceeds 10% of system memory.\n"
     ]
    }
   ],
   "source": [
    "parquetFile = dataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip'\n",
    "df = pd.read_parquet(parquetFile)\n",
    "labels = df.pop(\"genMass\")    \n",
    "features = df.values\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(dataset)\n",
    "print(labels)\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
