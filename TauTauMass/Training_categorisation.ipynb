{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup. Libraries loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "workerEnv = str(os.getenv(\"USER\"))\n",
    "workOnPrometheus = workerEnv.find(\"plg\")>-1\n",
    "inputDataPrefix = \"/home/user1/scratch_ssd/akalinow/\"\n",
    "if workOnPrometheus:\n",
    "    inputDataPrefix = \"/net/people/plgakalinow/plggcmsml/\"\n",
    "\n",
    "trainDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/TauTauMass/data/15_09_2020/\"   \n",
    "testDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/TauTauMass/data/29_06_2020/\" \n",
    "\n",
    "massBins = tf.concat((tf.range(0.0, 300, 5, dtype=tf.float32), [300,9999.0]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit --port=8008 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_2d(tensor):\n",
    "\n",
    "  # flatten the Tensor along the height and width axes\n",
    "  flat_tensor = tf.reshape(tensor, (-1))\n",
    "\n",
    "  # argmax of the flat tensor\n",
    "  argmax = tf.cast(tf.argmax(flat_tensor, axis=0), tf.int32)\n",
    "\n",
    "  # convert indexes into 2D coordinates\n",
    "  argmax_x = argmax // tf.shape(tensor)[1]\n",
    "  argmax_y = argmax % tf.shape(tensor)[1]\n",
    "\n",
    "  # stack and return 2D coordinates\n",
    "  return tf.stack((argmax_x, argmax_y), axis=0)\n",
    "\n",
    "\n",
    "def finalModelAnswer(predictions):\n",
    "    #norm = np.sum(predictions, axis=1)\n",
    "    predictions = np.cumsum(predictions, axis=1)>0.5\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    mass = label2Mass(predictions)\n",
    "    return mass\n",
    "\n",
    "\n",
    "def smearedMETgenerator(dataset, gridSize):  \n",
    "     for dataRow in dataset:\n",
    "      smearedBatch = generate_smeared_batch(dataRow, gridSize)\n",
    "      smearedBatchSize = smearedBatch.shape[0]\n",
    "      labels = tf.broadcast_to(dataRow[1], (smearedBatchSize, 1))\n",
    "      fastMTTPredictions = tf.broadcast_to(dataRow[2], (smearedBatchSize, 1))\n",
    "      original_met = dataRow[0][0,-3:-1] \n",
    "      covariance = tf.transpose(dataRow[3])\n",
    "      covariance = tf.broadcast_to(covariance, (smearedBatchSize, 4))   \n",
    "      yield (smearedBatch, labels, fastMTTPredictions, covariance, original_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativePosteriorCut = 0.2\n",
    "\n",
    "def plotPosterior(massGen, labels, predictions, indices):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    #TEST indices = np.logical_and(labels>massGen-2, labels<massGen+2)\n",
    "    predictions = predictions[indices]\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "    maxPosterior = tf.math.reduce_max(predictions)\n",
    "    scaleFactor = int(0.8/maxPosterior + 0.5)\n",
    "    scaleFactor = 5.0 # TEST\n",
    "    axes[0].plot(label2Mass(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    "    axes[0].plot(label2Mass(np.arange(predictions.shape[0])), np.cumsum(predictions), linestyle='-.',label=\"cumulative posterior\")\n",
    "    axes[0].axvline(massGen, linestyle='-', color=\"olivedrab\", label=r'$m^{GEN} $')\n",
    "    axes[1].plot(label2Mass(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    " \n",
    "    axes[0].set_xlabel(r'$m [GeV/c^{2}]$')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].set_xlim([0, 2*massGen])\n",
    "    axes[0].set_ylim([1E-3,1.05])    \n",
    "    axes[0].legend(bbox_to_anchor=(2.5,1), loc='upper left', title = r'$m^{GEN} = $'+str(massGen)+r'$~GeV/c^{2}$')\n",
    "    \n",
    "    axes[1].set_xlabel(r'$m~[GeV/c^{2}]$')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_xlim([0,300])\n",
    "    axes[1].set_ylim([1E-3,1.05])\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.3)\n",
    "    plt.savefig(\"fig_png/Posterior_massGen_{}.png\".format(massGen), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotPull(labels, predictions, fastMTTPredictions):\n",
    "    \n",
    "    minX = -1\n",
    "    maxX = 1\n",
    "    nBins = 50   \n",
    "    nnMTTPredictions = finalModelAnswer(predictions)\n",
    "     \n",
    "    error = (nnMTTPredictions- labels)/labels\n",
    "    fastMTTError = (fastMTTPredictions - labels)/labels    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 5))  \n",
    "    \n",
    "    axes[0].hist(error.numpy(), range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[0].hist(fastMTTError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"fastMTT\")  \n",
    "    axes[0].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].set_xlim([minX, maxX])\n",
    "    #axes[0].set_ylim([-2,2])\n",
    "    \n",
    "    axes[1].hist(fastMTTError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"fastMTT\")\n",
    "    axes[1].hist(error.numpy(), range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[1].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_xlim([minX, maxX])\n",
    "    plt.savefig(\"fig_png/Pull.png\", bbox_inches=\"tight\")\n",
    " \n",
    "def plotCM(labels, predictions, fastMTTPredictions):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize = (10, 10))  \n",
    "    \n",
    "    massMax =  massBins.shape[0]  \n",
    "    vmax = 1.0  \n",
    "    nnMTTPredictions = finalModelAnswer(predictions)\n",
    "    cm = tf.math.confusion_matrix(mass2Label(labels), mass2Label(nnMTTPredictions))\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    \n",
    "    myPalette = sns.color_palette(\"YlGnBu\", n_colors=20)\n",
    "    myPalette[0] = (1,1,1)\n",
    "    \n",
    "    vmax = 0.5 #TEST\n",
    "    sns.heatmap(cm, ax = axes[0,0], vmax = vmax, annot=False, xticklabels=10, yticklabels=10, cmap=myPalette)\n",
    "    axes[0,0].set_ylabel(r'$mass^{NN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].set_xlabel(r'$mass^{GEN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].grid()\n",
    "    axes[0,0].set_ylim([0,massMax])\n",
    "    axes[0,0].set_xlim([0,massMax])\n",
    "    axes[0,0].set_aspect(aspect='equal')\n",
    "    axes[0,0].set_title(\"NN\")\n",
    "    \n",
    "    cm = tf.math.confusion_matrix(mass2Label(labels), mass2Label(fastMTTPredictions))\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    #vmax = tf.math.reduce_max(cm)\n",
    "    sns.heatmap(cm, ax = axes[0,1], vmax = vmax, annot=False, xticklabels=10, yticklabels=10, cmap=myPalette)\n",
    "    axes[0,1].grid()\n",
    "    axes[0,1].set_title(\"fastMTT\")\n",
    "    axes[0,1].set_xlim([0,massMax])\n",
    "    axes[0,1].set_ylim([0,massMax])\n",
    "    axes[0,1].set_aspect(aspect='equal')\n",
    "    axes[0,1].set_ylabel(r'$mass^{fastMTT} \\rm{[bin ~number]}$')\n",
    "    axes[0,1].set_xlabel(r'$mass^{GEN} \\rm{[bin ~number]}$') \n",
    "    \n",
    "    axes[1,0].set_axis_off()\n",
    "    axes[1,1].set_axis_off() \n",
    "           \n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.25, hspace=0.35)\n",
    "    plt.savefig(\"fig_png/CM.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def compareDYandH125(labels, predictions, fastMTTPredictions):\n",
    "        \n",
    "    nnMTTPredictions = finalModelAnswer(predictions)\n",
    "    \n",
    "    indicesZ90 = np.logical_and(labels>88, labels<92)\n",
    "    labelsZ90 = labels[indicesZ90]\n",
    "    result_NN_Z90 = nnMTTPredictions[indicesZ90]\n",
    "    result_fastMTT_Z90 = fastMTTPredictions[indicesZ90]\n",
    "    \n",
    "    indicesH125 = np.logical_and(labels>123, labels<127)    \n",
    "    labelsH125 = labels[indicesH125]\n",
    "    result_NN_H125 = nnMTTPredictions[indicesH125]\n",
    "    result_fastMTT_H125 = fastMTTPredictions[indicesH125]\n",
    "    \n",
    "    pull_NN_Z90 = (result_NN_Z90 - labelsZ90)/labelsZ90\n",
    "    pull_NN_H125 = (result_NN_H125 - labelsH125)/labelsH125\n",
    "    \n",
    "    pull_fastMTT_Z90 = (result_fastMTT_Z90 - labelsZ90)/labelsZ90\n",
    "    pull_fastMTT_H125 = (result_fastMTT_H125 - labelsH125)/labelsH125\n",
    "    \n",
    "    print(\"fastMTT:\")\n",
    "    print(\"Mass range: Z90\",\n",
    "          \"mean pull: {0:3.3f}\".format(np.mean(pull_fastMTT_Z90)),\n",
    "          \"pull RMS: {0:3.3f} RMS/90: {1:3.4f}\".format(np.std(pull_fastMTT_Z90, ddof=1), np.std(pull_fastMTT_Z90, ddof=1)/90.0)\n",
    "         )\n",
    "    print(\"Mass range: H125\",\n",
    "          \"mean pull: {0:3.3f}\".format(np.mean(pull_fastMTT_H125)),\n",
    "          \"pull RMS: {0:3.3f} RMS/125: {1:3.4f}\".format(np.std(pull_fastMTT_H125, ddof=1), np.std(pull_fastMTT_H125, ddof=1)/125.0)\n",
    "         )  \n",
    "    print(\"NN:\")\n",
    "    print(\"Mass range: Z90\",\n",
    "          \"mean pull: {0:3.3f}\".format(np.mean(pull_NN_Z90)),\n",
    "          \"pull RMS: {0:3.3f} RMS/90: {1:3.4f}\".format(np.std(pull_NN_Z90, ddof=1), np.std(pull_NN_Z90, ddof=1)/90.0)\n",
    "         )\n",
    "    print(\"Mass range: H125\",\n",
    "          \"mean pull: {0:3.3f}\".format(np.mean(pull_NN_H125)),\n",
    "          \"pull RMS: {0:3.3f} RMS/125: {1:3.4f}\".format(np.std(pull_NN_H125, ddof=1), np.std(pull_NN_H125, ddof=1)/125.0)\n",
    "         )\n",
    "    \n",
    "    scores = np.concatenate((result_fastMTT_H125, result_fastMTT_Z90))\n",
    "    labels_S = np.ones(len(result_fastMTT_H125))\n",
    "    labels_B = np.zeros(len(result_fastMTT_Z90))\n",
    "    labels_S_B = np.concatenate((labels_S, labels_B))\n",
    "    fpr_fastMTT, tpr_fastMTT, thresholds_fastMTT = roc_curve(labels_S_B, scores, pos_label=1) \n",
    "    \n",
    "    scores = np.concatenate((result_NN_H125, result_NN_Z90))\n",
    "    labels_S = np.ones(len(result_NN_H125))\n",
    "    labels_B = np.zeros(len(result_NN_Z90))\n",
    "    labels_S_B = np.concatenate((labels_S, labels_B))\n",
    "    fpr_NN, tpr_NN, thresholds_NN = roc_curve(labels_S_B, scores, pos_label=1) \n",
    "    \n",
    "    minX = 0\n",
    "    maxX = 200\n",
    "    maxY = 0.9*np.maximum(result_NN_Z90.shape[0], result_NN_H125.shape[0])\n",
    "    nBins = 40\n",
    "    fig, axes = plt.subplots(1, 3, figsize = (12, 5))  \n",
    "    axes[0].hist(result_NN_Z90.numpy(), range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"m=90\")\n",
    "    axes[0].hist(result_NN_H125.numpy(), range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"m=125\")\n",
    "    axes[0].set_xlabel(\"Mass\")\n",
    "    axes[0].set_title(\"NN\")\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].set_xlim([minX, maxX])  \n",
    "    axes[0].set_ylim([0, maxY])  \n",
    "    \n",
    "    axes[1].hist(result_fastMTT_Z90, range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"m=90\")\n",
    "    axes[1].hist(result_fastMTT_H125, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"m=125\")\n",
    "    axes[1].set_xlabel(\"Mass\")\n",
    "    axes[1].set_title(\"fastMTT\")\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_xlim([minX, maxX]) \n",
    "    axes[1].set_ylim([0, maxY])\n",
    "     \n",
    "    axes[2].plot(tpr_NN, fpr_NN, label='NN')\n",
    "    axes[2].plot(tpr_fastMTT, fpr_fastMTT, label='fastMTT')\n",
    "    for x, y, txt in zip(tpr_NN[::1], fpr_NN[::1], thresholds_NN[::1]):\n",
    "        if x>0.9 and x<0.995:\n",
    "            axes[2].annotate(np.round(txt,2), (x, y-0.04), color=\"brown\")\n",
    "            \n",
    "    step = 10        \n",
    "    for x, y, txt in zip(tpr_fastMTT[::step], fpr_fastMTT[::step], thresholds_fastMTT[::step]):\n",
    "        if x>0.9 and x<0.995:\n",
    "            axes[2].annotate(np.round(txt,2), (x, y+0.04), color=\"blue\")       \n",
    "    \n",
    "    axes[2].set_xlim(0.90,1.0)\n",
    "    #axes[2].set_ylim(0.0,1.0)\n",
    "    axes[2].set_xlabel('True positive rate')\n",
    "    axes[2].set_ylabel('False positive rate')\n",
    "    axes[2].set_title('ROC curve')\n",
    "    axes[2].legend(loc='best')\n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.4, hspace=0.35)\n",
    "    plt.savefig(\"fig_png/ROC.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotMET(smeared_met, original_met, covariance): \n",
    "    \n",
    "  metX = smeared_met[:,0]\n",
    "  metY = smeared_met[:,1]\n",
    "  fig, axes = plt.subplots(2, 3, figsize = (10, 10))   \n",
    "  met = np.sqrt(metX**2 + metY**2)\n",
    "  binWidth = 5  \n",
    "  metBins = tf.range(0.0,200,binWidth)  \n",
    "  histogram = tfp.stats.histogram(met, metBins, extend_lower_interval=True, extend_upper_interval=True, dtype=tf.int32) \n",
    "  axes[0,0].bar(metBins[:-1], histogram, width=binWidth)\n",
    "  metBins = tf.range(-200.0,200,binWidth)  \n",
    "  histogram = tfp.stats.histogram(metX, metBins, extend_lower_interval=True, extend_upper_interval=True, dtype=tf.int32) \n",
    "  axes[0,1].bar(metBins[:-1], histogram, width=binWidth)\n",
    "  histogram = tfp.stats.histogram(metY, metBins, extend_lower_interval=True, extend_upper_interval=True, dtype=tf.int32) \n",
    "  axes[0,2].bar(metBins[:-1], histogram, width=binWidth)\n",
    "  axes[0,0].set_xlabel(r'$Total MET$')\n",
    "  axes[0,1].set_xlabel(r'$MET_{x}$') \n",
    "  axes[0,2].set_xlabel(r'$MET_{y}$') \n",
    "    \n",
    "  sns.heatmap(covariance, ax=axes[1,0], annot=True) \n",
    "  axes[1,0].set_title(r'$MET covariance$')\n",
    "  axes[1,0].set_xticklabels([r'$MET_{x}$', r'$MET_{y}$'])\n",
    "  axes[1,0].set_yticklabels([r'$MET_{x}$', r'$MET_{y}$']) \n",
    "    \n",
    "  metX -= original_met[0]\n",
    "  metY -= original_met[1]\n",
    "     \n",
    "  x, y = np.mgrid[-100:100:5, -100:100:5]\n",
    "  x_y = np.dstack((x, y))\n",
    "  gauss2D = scipy.stats.multivariate_normal(mean=[0,0], cov=covariance)\n",
    "  colorScale = axes[1,2].contourf(x, y, gauss2D.pdf(x_y))  \n",
    "  fig.colorbar(colorScale, ax=axes[1,2])\n",
    "  axes[1,2].scatter(metX, metY, facecolor='red')\n",
    "  '''  \n",
    "  for count, item in enumerate(zip(metX, metY)):\n",
    "            axes[1,2].annotate(count, (item[0], item[1]), color=\"brown\")\n",
    "  '''  \n",
    "  axes[1,2].set_xlabel(r'$(MET^{smear} - MET^{gen})_{x}$') \n",
    "  axes[1,2].set_ylabel(r'$(MET^{smear} - MET^{gen})_{y}$') \n",
    "  axes[1,2].set_xlim([-50,50])\n",
    "  axes[1,2].set_ylim([-50,50]) \n",
    "  axes[1,1].set_axis_off()\n",
    "\n",
    "  plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.25, hspace=0.4)\n",
    "  plt.savefig(\"fig_png/smeared_MET.png\", bbox_inches=\"tight\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(['eventWeight', 'sampleType', 'genMass', 'visMass', 'caMass',\n",
    "       'fastMTTMass', 'covMET00', 'covMET01', 'covMET10', 'covMET11',\n",
    "       'leg_1_E', 'leg_1_pX', 'leg_1_pY', 'leg_1_pZ', 'leg_2_E', 'leg_2_pX',\n",
    "       'leg_2_pY', 'leg_2_pZ', 'leg_3_E', 'leg_3_pX', 'leg_3_pY', 'leg_3_pZ',\n",
    "       'leg_4_E', 'leg_4_pX', 'leg_4_pY', 'leg_4_pZ', 'jet_1_E', 'jet_1_pX',\n",
    "       'jet_1_pY', 'jet_1_pZ', 'jet_2_E', 'jet_2_pX', 'jet_2_pY', 'jet_2_pZ',\n",
    "       'jet_3_E', 'jet_3_pX', 'jet_3_pY', 'jet_3_pZ', 'leg_2_decayMode'])\n",
    "                    \n",
    "def getFeaturesMask():\n",
    "    featuresMask = np.full_like(columns, True, dtype=np.bool)\n",
    "    featuresMask *= columns!=\"eventWeight\"  \n",
    "    featuresMask *= columns!=\"sampleType\"  \n",
    "    featuresMask *= columns!=\"genMass\"    \n",
    "    featuresMask *= columns!=\"visMass\"\n",
    "    featuresMask *= columns!=\"caMass\"\n",
    "    featuresMask *= columns!=\"fastMTTMass\" \n",
    "    featuresMask *= columns!=\"covMET00\"\n",
    "    featuresMask *= columns!=\"covMET01\"\n",
    "    featuresMask *= columns!=\"covMET10\"\n",
    "    featuresMask *= columns!=\"covMET11\"  \n",
    "    #Leg 1\n",
    "    #featuresMask *= columns!=\"leg_1_E\"\n",
    "    #featuresMask *= columns!=\"leg_1_pX\"\n",
    "    #featuresMask *= columns!=\"leg_1_pY\"\n",
    "    #featuresMask *= columns!=\"leg_1_pZ\"\n",
    "    #Leg 2\n",
    "    #featuresMask *= columns!=\"leg_2_E\"\n",
    "    #featuresMask *= columns!=\"leg_2_pX\"\n",
    "    #featuresMask *= columns!=\"leg_2_pY\"\n",
    "    #featuresMask *= columns!=\"leg_2_pZ\"\n",
    "    #featuresMask *= columns!=\"leg_2_decayMode\"\n",
    "    #MET passed as jet_1\n",
    "    #featuresMask *= columns!=\"jet_1_E\"\n",
    "    #featuresMask *= columns!=\"jet_1_pX\"\n",
    "    #featuresMask *= columns!=\"jet_1_pY\"\n",
    "    featuresMask *= columns!=\"jet_1_pZ\"\n",
    "    #Jets\n",
    "    featuresMask *= columns!=\"jet_2_E\"\n",
    "    featuresMask *= columns!=\"jet_2_pX\"\n",
    "    featuresMask *= columns!=\"jet_2_pY\"\n",
    "    featuresMask *= columns!=\"jet_2_pZ\"\n",
    "    featuresMask *= columns!=\"jet_3_E\"\n",
    "    featuresMask *= columns!=\"jet_3_pX\"\n",
    "    featuresMask *= columns!=\"jet_3_pY\"\n",
    "    featuresMask *= columns!=\"jet_3_pZ\" \n",
    "    #Generated legs\n",
    "    featuresMask *= columns!=\"leg_3_E\"\n",
    "    featuresMask *= columns!=\"leg_3_pX\"\n",
    "    featuresMask *= columns!=\"leg_3_pY\"\n",
    "    featuresMask *= columns!=\"leg_3_pZ\"\n",
    "    #\n",
    "    featuresMask *= columns!=\"leg_4_E\"\n",
    "    featuresMask *= columns!=\"leg_4_pX\"\n",
    "    featuresMask *= columns!=\"leg_4_pY\"\n",
    "    featuresMask *= columns!=\"leg_4_pZ\"\n",
    "    #\n",
    "    return featuresMask                   \n",
    "\n",
    "def mass2Label(tensor):\n",
    "    tensor = tf.searchsorted(massBins, tensor, side='left')\n",
    "    return tensor\n",
    "    \n",
    "def label2Mass(tensor): \n",
    "    massValues = tf.gather(massBins, tensor)\n",
    "    return tf.where(massValues<300, massValues, [300]) \n",
    "\n",
    "def getFeature(name, dataRow):\n",
    "    columnIndex = np.where(columns == name)[0][0]  \n",
    "    return dataRow[:,columnIndex]\n",
    "\n",
    "def modifyFeatures(dataRow, batchSize, isTrain=False):\n",
    "    columnsMask = getFeaturesMask()\n",
    "    features = tf.boolean_mask(dataRow, columnsMask, axis=1)\n",
    "    \n",
    "    fastMTTMass = getFeature(\"fastMTTMass\", dataRow)\n",
    "    genMass = getFeature(\"genMass\", dataRow)\n",
    "    visMass = getFeature(\"visMass\", dataRow)\n",
    "    labels = genMass\n",
    "    labels.set_shape([batchSize,])\n",
    "    \n",
    "    if isTrain:\n",
    "        labels = mass2Label(labels)\n",
    "        return (features, labels)\n",
    "    else:\n",
    "        fastMTT= getFeature(\"fastMTTMass\", dataRow)\n",
    "        fastMTT.set_shape([batchSize,])        \n",
    "        covariance = getCovariance(dataRow)\n",
    "        return (features, labels, fastMTT, covariance)\n",
    "    return dataRow\n",
    "    \n",
    "def loadDataset(filePaths, isTrain, nEpochs=1, batchSize=1):   \n",
    "    parquetFile = filePaths[0]\n",
    "    df = pd.read_parquet(parquetFile)\n",
    "    df = df.astype('float32')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dataset = dataset.batch(batchSize, drop_remainder=True)\n",
    "    dataset = dataset.map(lambda x: modifyFeatures(x, batchSize, isTrain)) \n",
    "    return dataset\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(1E-10)\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "    \n",
    "   \n",
    "def getCovariance(dataRow):\n",
    "    \n",
    "    #covScaling = 60000.0\n",
    "    covScaling = 1.0\n",
    "    cov00 = getFeature(\"covMET00\", dataRow)*covScaling\n",
    "    cov01 = getFeature(\"covMET01\", dataRow)\n",
    "    cov10 = getFeature(\"covMET10\", dataRow)\n",
    "    cov11 = getFeature(\"covMET11\", dataRow)*covScaling \n",
    "    return (cov00, cov01, cov10, cov11)\n",
    "\n",
    "def generate_smeared_batch(dataRow, gridSize=10):\n",
    "        cov = tf.reshape(dataRow[3],(-1,2,2))[0]\n",
    "\n",
    "        leg1_x_y = dataRow[0][0][2:4]\n",
    "        leg2_x_y = dataRow[0][0][6:8]\n",
    "        \n",
    "        #leg1_x_y = tf.constant([10,0], dtype=tf.float32) #TEST\n",
    "        #leg2_x_y = tf.constant([0,10], dtype=tf.float32) #TEST\n",
    "        \n",
    "        met = dataRow[0][0][-3:-1]  \n",
    "        \n",
    "        x1, x2 = tf.meshgrid(tf.linspace(0.01, 0.99, gridSize), tf.linspace(0.01, 0.99, gridSize))\n",
    "        x1 = tf.reshape(x1,(-1,1))\n",
    "        x2 = tf.reshape(x2,(-1,1))\n",
    "        \n",
    "        smeared_met = tf.broadcast_to(met, (gridSize*gridSize, 2))\n",
    "        leg1_x_y = tf.broadcast_to(leg1_x_y, (gridSize*gridSize, 2))\n",
    "        leg2_x_y = tf.broadcast_to(leg2_x_y, (gridSize*gridSize, 2))\n",
    "        x1 = tf.broadcast_to(x1, (x1.shape[0], 2))\n",
    "        x2 = tf.broadcast_to(x2, (x2.shape[0], 2))\n",
    "        smeared_met = (1/x1-1)*leg1_x_y + (1/x2-1)*leg2_x_y\n",
    "    \n",
    "        \n",
    "        smeared_met = np.random.default_rng().multivariate_normal(mean=met, cov=cov, size = gridSize*gridSize)#TEST\n",
    "        smeared_met = tf.cast(smeared_met, dtype=tf.float32)\n",
    "        \n",
    "        smeared_metTot = tf.math.sqrt(smeared_met[:,0]**2 + smeared_met[:,1]**2)\n",
    "        smeared_metTot = tf.reshape(smeared_metTot, (-1,1))\n",
    "        \n",
    "        features = dataRow[0][0][:]   \n",
    "        numberOfFeatures = features.shape[0]\n",
    "        features = tf.broadcast_to(features, (gridSize*gridSize,numberOfFeatures)) \n",
    "        decayModeColumn = features[:,-1]\n",
    "        decayModeColumn = tf.reshape(decayModeColumn, (gridSize*gridSize,1))  \n",
    "        dataRow = tf.concat([features[:,:-4], smeared_metTot, smeared_met, decayModeColumn], axis=1)  \n",
    "        '''\n",
    "        probabilityThreshold = 0.01\n",
    "        gauss2D = scipy.stats.multivariate_normal(mean=met, cov=covariance)\n",
    "        indices = gauss2D.pdf(smeared_met)>gauss2D.pdf(met)*probabilityThreshold\n",
    "        a = tf.boolean_mask(x1, indices, axis=0)[:,0]\n",
    "        b = tf.boolean_mask(x2, indices, axis=0)[:,0]\n",
    "        c = tf.stack([a,b, tf.sqrt(a*b)])\n",
    "        c = tf.transpose(c)\n",
    "        #print(\"x1,x2\",c)\n",
    "        dataRow = tf.boolean_mask(dataRow, indices, axis=0)\n",
    "        '''\n",
    "        return dataRow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainFileNames = glob.glob(trainDataDir + 'df.parquet_htt_features_train.gzip')\n",
    "trainFileNames = glob.glob(trainDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip')\n",
    "#trainFileNames = glob.glob(trainDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8_smearMET.gzip')\n",
    "#trainFileNames = glob.glob(trainDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_DY_ggH125.gzip')\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=False, nEpochs=1, batchSize=1)\n",
    "\n",
    "#dataset = train_dataset\n",
    "#benchmark(dataset)\n",
    "#benchmark(dataset.prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "for dataRow in train_dataset.take(1):\n",
    "  #print(dataRow) \n",
    "  x = generate_smeared_batch(dataRow, gridSize=1)\n",
    "  print(\"dataRow\",dataRow)  \n",
    "  print(\"smeared:\",x)  \n",
    "  break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_mean_metric(y_true, y_pred):    \n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Mass(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Mass(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels   \n",
    "    mean = tf.math.reduce_mean(pull, axis=0)\n",
    "    return mean \n",
    "    \n",
    "def pull_variance_metric(y_true, y_pred):\n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Mass(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Mass(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels  \n",
    "    variance = tf.math.reduce_variance(pull, axis=0) \n",
    "    return tf.sqrt(variance) \n",
    "\n",
    "custom_objects={'pull_mean_metric': pull_mean_metric,\n",
    "                'pull_variance_metric':pull_variance_metric\n",
    "               }\n",
    "\n",
    "  \n",
    "def getModel():\n",
    "    \n",
    "    nMassBins =  massBins.shape[0]\n",
    "    nInputs = np.sum(getFeaturesMask())\n",
    "    inputs = tf.keras.Input(shape=(nInputs,), name=\"features\")\n",
    "    x = tf.keras.backend.cast(inputs, tf.float32)\n",
    "    activation = tf.keras.layers.Activation(tf.nn.relu)\n",
    "    for iLayer in range(0,20):\n",
    "        x = tf.keras.layers.Dense(128, activation=activation, name=\"layer_{}\".format(iLayer))(x)\n",
    "            \n",
    "    outputs = tf.keras.layers.Dense(nMassBins, activation=tf.nn.softmax,\n",
    "                                   bias_initializer='zeros',\n",
    "                                   name = \"mass\")(x)              \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"NN_SVFit\")\n",
    "    \n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                                decay_steps=1000,\n",
    "                                                                decay_rate=0.95,\n",
    "                                                                staircase=True)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    tf.keras.utils.plot_model(model, 'fig_png/ML_model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start. Current Time = Nov 12 2020 15:03:12\n",
      "Epoch 1/5\n",
      "245/250 [============================>.] - ETA: 0s - loss: 3.2744 - accuracy: 0.1011WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f88b02af598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 3.2686 - accuracy: 0.1018 - val_loss: 3.2726 - val_accuracy: 0.0957\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.9217 - accuracy: 0.1447 - val_loss: 3.1120 - val_accuracy: 0.1142\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 2.7154 - accuracy: 0.1722 - val_loss: 2.8803 - val_accuracy: 0.1434\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.5410 - accuracy: 0.1984 - val_loss: 2.6446 - val_accuracy: 0.1801\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.3952 - accuracy: 0.2212 - val_loss: 2.7363 - val_accuracy: 0.1581\n",
      "INFO:tensorflow:Assets written to: training/model_0005/assets\n",
      "Training end. Current Time = Nov 12 2020 15:03:32\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training start. Current Time =\", current_time)\n",
    "\n",
    "trainFileNames = glob.glob(trainDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip')\n",
    "#trainFileNames = glob.glob(dataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8_smearMET.gzip')\n",
    "#trainFileNames = glob.glob(dataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_DY_ggH125.gzip')\n",
    "testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip')\n",
    "\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=True, nEpochs=1, batchSize=8*1024)\n",
    "validation_dataset = loadDataset(testFileNames, isTrain=True, nEpochs=1, batchSize=16*1024)\n",
    "\n",
    "model = getModel()\n",
    "#nEpochsSaved = 10\n",
    "#checkpoint_path = \"training/model_weights_{epoch:04d}.ckmass\"\n",
    "#model.load_weights(checkpoint_path.format(epoch=nEpochsSaved)) \n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=2)\n",
    "\n",
    "nEpochs = 5\n",
    "history = model.fit(train_dataset, epochs=nEpochs,\n",
    "                    use_multiprocessing=True,\n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    validation_data=validation_dataset.take(1),\n",
    "                    #callbacks=[tensorboard_callback]\n",
    "                   )\n",
    "\n",
    "# Save the whole model\n",
    "path = \"training/model_{epoch:04d}\"\n",
    "model.save(path.format(epoch=nEpochs), save_format='tf')\n",
    "#Save model weights\n",
    "path = \"training/model_weights_{epoch:04d}.ckpt\"\n",
    "model.save_weights(path.format(epoch=nEpochs))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training end. Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochsSaved = 50\n",
    "checkpoint_path = \"training/model_{epoch:04d}\"\n",
    "model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not workOnPrometheus:\n",
    "    nEpochsSaved = 250    \n",
    "    checkpoint_path = \"training/model_{epoch:04d}\"\n",
    "    model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "       \n",
    "    current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "\n",
    "    testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip') \n",
    "    #testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8_smearMET.gzip')\n",
    "    #testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_DY_ggH125.gzip')\n",
    "    test_dataset = loadDataset(testFileNames, isTrain=False, nEpochs=1, batchSize=50000)\n",
    "    for aBatch in test_dataset.take(1).as_numpy_iterator():            \n",
    "            labels = aBatch[1]  \n",
    "            fastMTTPredictions = aBatch[2] \n",
    "            features = aBatch[0]\n",
    "            predictions = model.predict(features, use_multiprocessing=True) \n",
    "            plotPull(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)\n",
    "            plotCM(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)\n",
    "            compareDYandH125(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply MET smearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8.gzip') \n",
    "#testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_Pythia8_smearMET.gzip')\n",
    "testFileNames = glob.glob(testDataDir + 'df.parquet_RootAnalysis_SVfitMLAnalysisMuTau_DY_ggH125.gzip')\n",
    "test_dataset = loadDataset(testFileNames, isTrain=False, nEpochs=1, batchSize=1)\n",
    "\n",
    "generator = partial(smearedMETgenerator, test_dataset, 10)  \n",
    "smeared_Dataset = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))\n",
    "     \n",
    "nnMTT = np.zeros_like(massBins)\n",
    "fastMTT = np.array([])\n",
    "truth = np.array([])   \n",
    "\n",
    "nEpochsSaved = 200\n",
    "checkpoint_path = \"training/model_{epoch:04d}\"\n",
    "model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "    \n",
    "for aBatch in smeared_Dataset.take(2000).as_numpy_iterator():\n",
    "    labels = np.reshape(aBatch[1], [-1])\n",
    "    fastMTTPredictions = np.reshape(aBatch[2], [-1])\n",
    "    features = aBatch[0]\n",
    "    original_met = np.reshape(aBatch[4:5], (2))\n",
    "    smeared_met = np.reshape(features[:,-3:-1], (-1,2))\n",
    "    predictions = model.predict(features, use_multiprocessing=True) \n",
    "    \n",
    "    covariance = np.array(aBatch[3][0])\n",
    "    covariance = np.reshape(covariance, (2,2)) \n",
    "    \n",
    "    gauss2D = scipy.stats.multivariate_normal(mean=original_met, cov=covariance)\n",
    "    met_tf_weights = gauss2D.pdf(smeared_met)\n",
    "    met_tf_weights = tf.reshape(met_tf_weights, (-1,1))\n",
    "    met_tf_weights = met_tf_weights/gauss2D.pdf(original_met)\n",
    "    \n",
    "    predictions_with_weights = tf.math.multiply(predictions, met_tf_weights)\n",
    "    resultIndex = argmax_2d(predictions_with_weights)\n",
    "    resultValue = label2Mass(resultIndex[1])\n",
    "    \n",
    "    nnMTT = np.vstack((nnMTT,predictions_with_weights[resultIndex[0]]))\n",
    "    fastMTT = np.append(fastMTT, fastMTTPredictions[0])\n",
    "    truth = np.append(truth, labels[0])\n",
    "    \n",
    "    '''\n",
    "    print(\"label\",labels[0])\n",
    "    print(\"predictions.shape\",predictions.shape)\n",
    "    print(\"met_tf_weights.shape\",met_tf_weights.shape)\n",
    "    print(\"resultIndex\",resultIndex)\n",
    "    print(\"resultIndex[0]\",resultIndex.numpy()[0])\n",
    "    print(\"best weight\",met_tf_weights[resultIndex.numpy()[0]])\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #plotMET(smeared_met, original_met, covariance)  \n",
    "    #plotPull(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)\n",
    "    #plotPull(labels=labels, predictions=predictions_with_weights, fastMTTPredictions=fastMTTPredictions)\n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[resultIndex.numpy()[0]]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[1]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[2]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[3]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[4]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[5]) \n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[6])\n",
    "    #plotPosterior(massGen = labels[0], labels=labels, predictions=predictions, indices=[8])\n",
    "    #plotPull(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)\n",
    "    #plotCM(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)\n",
    "    #compareDYandH125(labels=labels, predictions=predictions, fastMTTPredictions=fastMTTPredictions)  \n",
    "    \n",
    " \n",
    "nnMTT = nnMTT[1:,:] \n",
    "plotPull(labels=truth, predictions=nnMTT, fastMTTPredictions=fastMTT)\n",
    "compareDYandH125(labels=truth, predictions=nnMTT, fastMTTPredictions=fastMTT) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
