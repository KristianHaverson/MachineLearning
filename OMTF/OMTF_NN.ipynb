{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMTF definitions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "inputDataPrefix = \"\"\n",
    "\n",
    "SLURM_CLUSTER_NAME=str(os.getenv(\"SLURM_CLUSTER_NAME\"))\n",
    "runOnCluster = False\n",
    "if SLURM_CLUSTER_NAME == \"prometheus\":\n",
    "    inputDataPrefix = \"/net/people/plgakalinow/plggcmsml/\"\n",
    "    runOnCluster = True\n",
    "elif SLURM_CLUSTER_NAME == \"rysy\":   \n",
    "    inputDataPrefix = \"/home/akalinow/\"\n",
    "    runOnCluster = True\n",
    "else: \n",
    "    inputDataPrefix =  \"/home/user1/scratch_ssd/akalinow/\"\n",
    "    runOnCluster = False\n",
    "    \n",
    "trainDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/OMTF/data/18_12_2020/\"   \n",
    "testDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/OMTF/data/18_12_2020/\" \n",
    "\n",
    "nRefLayers = 8\n",
    "nLayers = 18\n",
    "nPDFBins = 2**7\n",
    "minProbability = 0.001\n",
    "minPlog = np.log(minProbability)\n",
    "nPdfValBits = 6\n",
    "refLayers = [0, 7, 2, 6, 16, 4, 10, 11]\n",
    "ptBins = tf.concat([tf.range(0.0, 5, 0.5), tf.range(5, 30, 1.0, ), tf.range(30, 40, 5.0), tf.range(40, 60, 10.0), tf.range(60, 101, 20.0), [99999.0]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use mixed precision policy: 32 bits for data, 16 bits for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not runOnCluster:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir logs/fit --port=8008 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativePosteriorCut = 0.70\n",
    "testIndex = 0\n",
    "\n",
    "def plotPosterior(ptGen, labels, predictions):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    indices = np.logical_and(labels>ptGen-0.1, labels<ptGen+0.1)\n",
    "        \n",
    "    predictions = predictions[indices]\n",
    "    \n",
    "    ###TEST\n",
    "    #predictions = predictions[:,0:45]\n",
    "    predictions = predictions[testIndex] \n",
    "    predictions = tf.reshape(predictions, (1,-1))\n",
    "    derivative = np.diff(predictions, append=0)\n",
    "    #predictions = tf.math.divide_no_nan(derivative, predictions)\n",
    "    predictions = tf.abs(predictions)\n",
    "    #sum = tf.reduce_sum(predictions, axis=1, keepdims=True)\n",
    "    #predictions = 1.0/sum*predictions\n",
    "    ######\n",
    "    \n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "    maxPosterior = tf.math.reduce_max(predictions)\n",
    "    scaleFactor = int(0.8/maxPosterior + 0.5)\n",
    "    axes[0].plot(label2Pt(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    "    axes[0].plot(label2Pt(np.arange(predictions.shape[0])), np.cumsum(predictions), linestyle='-.',label=\"cumulative posterior\")\n",
    "    axes[1].plot(label2Pt(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    "    \n",
    "    predictions = np.cumsum(predictions, axis=0)>cumulativePosteriorCut\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    ptRec = label2Pt(predictions)\n",
    "    print(\"Pt gen = {}, Pt rec {} cumulative posterior: {}\".format(ptGen, cumulativePosteriorCut, ptRec))\n",
    "    axes[0].axvline(ptGen, linestyle='-', color=\"olivedrab\", label=r'$p_{T}^{GEN} \\pm 1 [GeV/c]$')\n",
    "    axes[0].axvline(ptRec, linestyle='--', color=\"r\", label=r'$p_{T}^{REC} @ cum~post.=$'+str(cumulativePosteriorCut))\n",
    "    \n",
    "    axes[0].set_xlabel(r'$p_{T} [GeV/c]$')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].set_xlim([0, 2*ptGen])\n",
    "    axes[0].set_ylim([1E-3,1.05])\n",
    "    \n",
    "    axes[0].legend(bbox_to_anchor=(2.5,1), loc='upper left')\n",
    "    axes[1].set_xlabel(r'$p_{T} [GeV/c]$')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_xlim([0,201])\n",
    "    axes[1].set_ylim([1E-3,1.05])\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.3)\n",
    "    plt.savefig(\"fig_png/Posterior_ptGen_{}.png\".format(ptGen), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotTurnOn(dataset, ptCut):\n",
    "    ptMax = ptCut+50\n",
    "    #ptMax = 10 #TEST\n",
    "    nPtBins = int(ptMax*2.0)\n",
    "    ptHistoBins = range(0,nPtBins+1)\n",
    "\n",
    "    num = np.zeros(nPtBins)\n",
    "    numML = np.zeros(nPtBins)\n",
    "    denom = np.zeros(nPtBins)\n",
    "    \n",
    "    count =0\n",
    "    for aBatch in dataset.as_numpy_iterator():\n",
    "        labels = aBatch[1][0]\n",
    "        omtfPredictions = aBatch[2]\n",
    "        count += labels.shape[0]\n",
    "        predictions = model.predict(aBatch[0], use_multiprocessing=True)\n",
    "        predictions = predictions[0]\n",
    "        predictions = predictions[:,0:45] #TEST\n",
    "        predictions = np.cumsum(predictions, axis=1)>cumulativePosteriorCut\n",
    "        predictions = np.argmax(predictions, axis=1)   \n",
    "        predictions = label2Pt(predictions)\n",
    "        \n",
    "        tmp,_ = np.histogram(labels, bins=ptHistoBins)    \n",
    "        denom +=tmp\n",
    "        tmp,_ = np.histogram(labels[omtfPredictions>=ptCut], bins=ptHistoBins)\n",
    "        num += tmp\n",
    "        tmp,_ = np.histogram(labels[predictions>=ptCut], bins=ptHistoBins)\n",
    "        numML += tmp\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    ratio = np.divide(num, denom, out=np.zeros_like(denom), where=denom>0)\n",
    "    ratioML = np.divide(numML, denom, out=np.zeros_like(denom), where=denom>0)\n",
    "    axes[0].plot(ptHistoBins[:-1],num, label=\"OMTF\")\n",
    "    axes[0].plot(ptHistoBins[:-1],numML, label=\"ML\")\n",
    "    axes[0].set_xlim([0,2.0*ptCut])\n",
    "    #axes[0].set_ylim([0,1.0])\n",
    "    axes[0].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[0].set_ylabel('Events passing pT cut')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    \n",
    "    axes[1].plot(ptHistoBins[:-1],ratio, label=\"OMTF\")\n",
    "    axes[1].plot(ptHistoBins[:-1],ratioML, label=\"ML\")\n",
    "    axes[1].grid()\n",
    "    axes[1].set_yscale(\"log\")\n",
    "    axes[1].set_xlim([0,ptMax])\n",
    "    axes[1].set_ylim([1E-3,1.05])\n",
    "    axes[1].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[1].set_ylabel('Efficiency')\n",
    "\n",
    "    axes[2].plot(ptHistoBins[:-1],ratio, label=\"OMTF\")\n",
    "    axes[2].plot(ptHistoBins[:-1],ratioML, label=\"ML\")\n",
    "    axes[2].grid()\n",
    "    axes[2].axhline(y=0.5)\n",
    "    axes[2].axhline(y=0.85)\n",
    "    axes[2].axvline(x=ptCut)\n",
    "    axes[2].set_xlim([0,ptMax])\n",
    "    axes[2].set_ylim([0.0,1.05])\n",
    "    axes[2].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[2].set_ylabel('Efficiency')\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.5)\n",
    "    plt.savefig(\"fig_png/TurnOn_ptCut_{}.png\".format(ptCut), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotPull(labels, predictions, omtfPredictions):\n",
    "    \n",
    "    minX = -1\n",
    "    maxX = 2\n",
    "    nBins = 50\n",
    "    predictions = np.cumsum(predictions, axis=1)>cumulativePosteriorCut\n",
    "    predictions = np.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)   \n",
    "    error = (predictions - labels)/labels\n",
    "    omtfError = (omtfPredictions - labels)/labels    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 5))  \n",
    "    axes[0].hist(error, range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[0].hist(omtfError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"OMTF\")\n",
    "    axes[0].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].set_xlim([minX, maxX])\n",
    "    #axes[0].set_ylim([-2,2])\n",
    "    \n",
    "    axes[1].hist(omtfError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"OMTF\")\n",
    "    axes[1].hist(error, range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[1].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_xlim([minX, maxX])\n",
    "    plt.savefig(\"fig_png/Pull.png\", bbox_inches=\"tight\")\n",
    " \n",
    "def plotCM(labels, predictions, omtfPredictions):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize = (10, 10))  \n",
    "    \n",
    "    ptMax =  ptBins.shape[0]  \n",
    "    vmax = 1.0\n",
    "    ptPredictions = np.cumsum(predictions[0], axis=1)>cumulativePosteriorCut\n",
    "    ptPredictions = np.argmax(ptPredictions, axis=1)   \n",
    "    cm = tf.math.confusion_matrix(pT2Label(labels[0]), ptPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    #vmax = tf.math.reduce_max(cm)\n",
    "    \n",
    "    myPalette = sns.color_palette(\"YlGnBu\", n_colors=20)\n",
    "    myPalette[0] = (1,1,1)\n",
    "    \n",
    "    vmax = 0.1 #TEST\n",
    "    sns.heatmap(cm, ax = axes[0,0], vmax = vmax, annot=False, xticklabels=4, yticklabels=4, cmap=myPalette)\n",
    "    axes[0,0].set_ylabel(r'$p_{T}^{NN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].set_xlabel(r'$p_{T}^{GEN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].grid()\n",
    "    axes[0,0].set_ylim([0,ptMax])\n",
    "    axes[0,0].set_xlim([0,ptMax])\n",
    "    axes[0,0].set_aspect(aspect='equal')\n",
    "    axes[0,0].set_title(\"NN\")\n",
    "    \n",
    "    cm = tf.math.confusion_matrix(pT2Label(labels[0]), pT2Label(omtfPredictions[0]))\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    #vmax = tf.math.reduce_max(cm)\n",
    "    sns.heatmap(cm, ax = axes[0,1], vmax = vmax, annot=False, xticklabels=4, yticklabels=4, cmap=myPalette)\n",
    "    axes[0,1].grid()\n",
    "    axes[0,1].set_title(\"OMTF\")\n",
    "    axes[0,1].set_xlim([0,ptMax])\n",
    "    axes[0,1].set_ylim([0,ptMax])\n",
    "    axes[0,1].set_aspect(aspect='equal')\n",
    "    axes[0,1].set_ylabel(r'$p_{T}^{OMTF} \\rm{[bin ~number]}$')\n",
    "    axes[0,1].set_xlabel(r'$p_{T}^{GEN} \\rm{[bin ~number]}$') \n",
    "        \n",
    "    chPredictions =tf.map_fn(lambda x: x>0.5, predictions[1], dtype=tf.bool)\n",
    "    chPredictions = tf.reshape(chPredictions, (-1,1))\n",
    "    chLabels =tf.map_fn(lambda x: x>0.5, labels[1], dtype=tf.bool)\n",
    "    #chLabels = tf.reshape(chLabels, (-1,1))\n",
    "    \n",
    "    vmax = 1.0\n",
    "    vmin = 0.0\n",
    "    cm = tf.math.confusion_matrix(chLabels, chPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    sns.heatmap(cm, ax = axes[1,0], vmax = vmax, annot=True, cmap=myPalette)\n",
    "    axes[1,0].set_title(\"NN\")\n",
    "    axes[1,0].set_aspect(aspect='equal')\n",
    "    axes[1,0].set_ylabel(r'$q^{NN}$')\n",
    "    axes[1,0].set_xlabel(r'$q^{GEN}$') \n",
    "    \n",
    "    chPredictions =tf.map_fn(lambda x: x>0.5, omtfPredictions[1], dtype=tf.bool)  \n",
    "    chPredictions = tf.reshape(chPredictions, (-1,1))\n",
    "    cm = tf.math.confusion_matrix(chLabels, chPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    sns.heatmap(cm, ax = axes[1,1], vmax = vmax, annot=True, cmap=myPalette, linewidths=0.01)\n",
    "    axes[1,1].set_title(\"OMTF\")\n",
    "    axes[1,1].set_aspect(aspect='equal')\n",
    "    axes[1,1].set_ylabel(r'$q^{NN}$')\n",
    "    axes[1,1].set_xlabel(r'$q^{GEN}$') \n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.25, hspace=0.35)\n",
    "    plt.savefig(\"fig_png/CM.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillPandasDataset(aBatch, df):\n",
    "    features = aBatch[0]\n",
    "    labels = aBatch[1]  \n",
    "    fastMTTPredictions = aBatch[2]\n",
    "    testMass = None\n",
    "     \n",
    "    predictions = model.predict(features, use_multiprocessing=True) \n",
    "    batch_df = pd.DataFrame(data={\"genPt\":labels, \"genCharge\":labels, \"OMTF\":fastMTTPredictions, \"NN\":finalModelAnswer(predictions, testMass)})\n",
    "    return df.append(batch_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(['muonPt', 'muonEta', 'muonPhi', 'muonCharge', 'omtfPt', 'omtfEta',\n",
    "       'omtfPhi', 'omtfCharge', 'omtfScore', 'omtfQuality', 'omtfRefLayer',\n",
    "       'omtfProcessor', 'omtfFiredLayers', 'phiDist_0', 'phiDist_1',\n",
    "       'phiDist_2', 'phiDist_3', 'phiDist_4', 'phiDist_5', 'phiDist_6',\n",
    "       'phiDist_7', 'phiDist_8', 'phiDist_9', 'phiDist_10', 'phiDist_11',\n",
    "       'phiDist_12', 'phiDist_13', 'phiDist_14', 'phiDist_15', 'phiDist_16',\n",
    "       'phiDist_17'])\n",
    "\n",
    "def getFeaturesMask():\n",
    "    featuresMask = np.full_like(columns, False, dtype=np.bool)\n",
    "    for iLayer in range(0, 18):\n",
    "        featureLabel = \"phiDist_{}\".format(iLayer)\n",
    "        featuresMask += (columns==featureLabel)\n",
    "    #featuresMask += columns==\"omtfFiredLayers\"\n",
    "    #featuresMask += columns==\"omtfRefLayer\"\n",
    "    #featuresMask += columns==\"omtfPt\"\n",
    "    #featuresMask = columns==\"omtfQuality\"\n",
    "    return featuresMask\n",
    "\n",
    "def getFeature(name, dataRow):\n",
    "    columnIndex = np.where(columns == name)[0][0]  \n",
    "    return dataRow[:,columnIndex]\n",
    "\n",
    "def parse_tensor(tensor):\n",
    "    return tf.io.parse_tensor(tensor, out_type=tf.float32)\n",
    "\n",
    "def pT2Label(tensor):\n",
    "    tensor = tf.searchsorted(ptBins, tensor, side='left')\n",
    "    return tensor\n",
    "    \n",
    "def label2Pt(tensor):  \n",
    "    return tf.where(ptBins.numpy()[tensor]<9999, ptBins.numpy()[tensor], [200])\n",
    "\n",
    "def modifyFeatures(dataRow, batchSize, isTrain=False):\n",
    "    columnsMask = getFeaturesMask()\n",
    "    features = tf.boolean_mask(dataRow, columnsMask, axis=1)\n",
    "    dummyValue = 128 #TEST\n",
    "    features = tf.where(features<9999, features, dummyValue) \n",
    "    features.set_shape([batchSize,np.count_nonzero(columnsMask)])\n",
    "    features = tf.one_hot(tf.cast(features+64, dtype=tf.int32), depth=128)#TEST\n",
    "    \n",
    "    columnIndex = np.where(columns == \"muonCharge\")[0][0]  \n",
    "    chargeLabels = (dataRow[:,columnIndex]+1)/2 \n",
    "    chargeLabels.set_shape([batchSize,])\n",
    "    \n",
    "    columnIndex = np.where(columns == \"muonPt\")[0][0]\n",
    "    ptLabels = dataRow[:,columnIndex]\n",
    "    ptLabels.set_shape([batchSize,])\n",
    "    trainWeight = 1.0 #tf.exp(-ptLabels/35.0)\n",
    "    \n",
    "    if isTrain:\n",
    "        ptLabels = pT2Label(ptLabels)\n",
    "        return (features, (ptLabels, chargeLabels), trainWeight)\n",
    "    else:\n",
    "        columnIndex = np.where(columns == \"omtfPt\")[0][0]  \n",
    "        omtfPt = dataRow[:,columnIndex]\n",
    "        columnIndex = np.where(columns == \"omtfQuality\")[0][0]  \n",
    "        omtfQuality = dataRow[:,columnIndex]\n",
    "        omtfPt = tf.where(omtfQuality>=12, omtfPt, 0) \n",
    "        omtfPt.set_shape([batchSize,])\n",
    "        \n",
    "        columnIndex = np.where(columns == \"omtfCharge\")[0][0]  \n",
    "        omtfCharge = dataRow[:,columnIndex]\n",
    "        omtfCharge.set_shape([batchSize,])\n",
    "        return (features, (ptLabels, chargeLabels), omtfPt, omtfCharge)\n",
    "    return dataRow\n",
    "    \n",
    "def loadDataset(fileNames, isTrain, nEpochs=1, batchSize=1):   \n",
    "    raw_dataset = tf.data.TFRecordDataset(fileNames, compression_type=\"GZIP\")\n",
    "    dataset = raw_dataset.map(parse_tensor,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batchSize, drop_remainder=True)\n",
    "    #Split data into [features, labels] and modify features\n",
    "    dataset = dataset.map(lambda x: modifyFeatures(x, batchSize, isTrain),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    #dataset = dataset.cache('/scratch/akalinow/data_cache/')\n",
    "    return dataset\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(1E-10)\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_mean_metric(y_true, y_pred):    \n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Pt(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels   \n",
    "    mean = tf.math.reduce_mean(pull, axis=0)\n",
    "    return mean \n",
    "    \n",
    "def pull_variance_metric(y_true, y_pred):\n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Pt(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels  \n",
    "    variance = tf.math.reduce_variance(pull, axis=0) \n",
    "    return tf.sqrt(variance) \n",
    "\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    \n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    #lowPtLoss = (labels<(10))*(predictions>(15))\n",
    "    labels = y_true\n",
    "\n",
    "    lowPtLoss = tf.math.logical_and(tf.math.less(labels, 10), tf.math.greater(predictions, 10))\n",
    "    lowPtLoss = (predictions-labels)*lowPtLoss\n",
    "    lowPtLoss = tf.cast(lowPtLoss, float32)\n",
    "    \n",
    "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true, y_pred) + lowPtLoss\n",
    "\n",
    "\n",
    "custom_objects={'pull_mean_metric': pull_mean_metric,\n",
    "                'pull_variance_metric':pull_variance_metric,\n",
    "                'my_loss_fn':my_loss_fn\n",
    "               }\n",
    "def getModel():\n",
    "    \n",
    "  nPtBins =  ptBins.shape[0]\n",
    "  nInputs = np.sum(getFeaturesMask())\n",
    "    \n",
    "  inputs = tf.keras.Input(shape=(nInputs,128), name=\"deltaPhi\")\n",
    "  #activation = tf.keras.activations.swish \n",
    "  activation = tf.keras.activations.relu \n",
    "  \n",
    "  ptLayer = tf.keras.layers.Flatten(input_shape=(nInputs, 128), name=\"Flatten_input_pt\")(inputs) \n",
    "  #ptLayer = inputs\n",
    "  for iLayer in range(0,10):\n",
    "            ptLayer = tf.keras.layers.Dense(512, activation=activation, name=\"Pt_layer_{}\".format(iLayer))(ptLayer)\n",
    "    \n",
    "  #chargeLayer= inputs  \n",
    "  chargeLayer = tf.keras.layers.Flatten(input_shape=(nInputs, 128), name=\"Flatten_input_charge\")(inputs) \n",
    "  for iLayer in range(0,1): \n",
    "       chargeLayer = tf.keras.layers.Dense(32, activation=activation, name=\"Charge_layer_{}\".format(iLayer))(chargeLayer)\n",
    "    \n",
    "  ptOutput = tf.keras.layers.Dense(nPtBins, activation=tf.nn.softmax,bias_initializer='zeros',name = \"pt\")(ptLayer) \n",
    "  chargeOutput = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid,name = \"charge\")(chargeLayer)\n",
    "        \n",
    "  model = tf.keras.Model(inputs=inputs, outputs=[ptOutput, chargeOutput], name=\"NN_OMTF\")\n",
    "\n",
    "  initial_learning_rate = 0.01\n",
    "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True)\n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss={\"pt\":tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                     \"charge\":tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "                     },                    \n",
    "                metrics=['accuracy'])\n",
    "  tf.keras.utils.plot_model(model, 'fig_png/ML_model.png', show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=True, nEpochs=1, batchSize=1)\n",
    "\n",
    "#dataset = train_dataset.take(10000)\n",
    "#benchmark(dataset)\n",
    "#benchmark(dataset.prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "#mask = tf.constant([0,1,2,3,10,11,12,13], dtype=tf.int32)\n",
    "#print(mask)\n",
    "\n",
    "for element in train_dataset.take(10): \n",
    "  print(element[0][0][17])\n",
    "  #x = tf.one_hot(tf.cast(element[0], dtype=tf.int32), depth=128)\n",
    "  #print(x.shape)  \n",
    "  #y = tf.gather(params=element[0], indices=mask, axis=1)\n",
    "  #print(y)  \n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start. Current Time = Jun 01 2021 09:15:04\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Epoch 1/3\n",
      "2542/2542 [==============================] - 244s 96ms/step - loss: 2.0761 - pt_loss: 2.0138 - charge_loss: 0.0623 - pt_accuracy: 0.3144 - charge_accuracy: 0.9762 - val_loss: 1.9361 - val_pt_loss: 1.8317 - val_charge_loss: 0.1043 - val_pt_accuracy: 0.3823 - val_charge_accuracy: 0.9688\n",
      "Epoch 2/3\n",
      "2542/2542 [==============================] - 230s 90ms/step - loss: 1.6894 - pt_loss: 1.6545 - charge_loss: 0.0348 - pt_accuracy: 0.3742 - charge_accuracy: 0.9844 - val_loss: 1.9121 - val_pt_loss: 1.8067 - val_charge_loss: 0.1054 - val_pt_accuracy: 0.3829 - val_charge_accuracy: 0.9685\n",
      "Epoch 3/3\n",
      "2542/2542 [==============================] - 230s 90ms/step - loss: 1.6756 - pt_loss: 1.6419 - charge_loss: 0.0337 - pt_accuracy: 0.3778 - charge_accuracy: 0.9848 - val_loss: 1.8942 - val_pt_loss: 1.7882 - val_charge_loss: 0.1060 - val_pt_accuracy: 0.3858 - val_charge_accuracy: 0.9682\n",
      "INFO:tensorflow:Assets written to: training/model_full_0003/assets\n",
      "Training end. Current Time = Jun 01 2021 09:26:51\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training start. Current Time =\", current_time)\n",
    "\n",
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_oldSample_files_*_chunk_0.tfrecord.gzip')\n",
    "validationFileNames = glob.glob(testDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')\n",
    "\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=True, nEpochs=1, batchSize=4*4096)\n",
    "validation_dataset = loadDataset(validationFileNames, isTrain=True, nEpochs=1, batchSize=1024)\n",
    "\n",
    "model = getModel()\n",
    "\n",
    "#nEpochsSaved = 5\n",
    "#checkpoint_path = \"training/model_full_{epoch:04d}\"\n",
    "#model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=(110, 120))\n",
    "#tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "\n",
    "nEpochs = 3\n",
    "history = model.fit(train_dataset, epochs=nEpochs,\n",
    "                            use_multiprocessing=True,\n",
    "                            verbose=1,\n",
    "                            shuffle=False,\n",
    "                            validation_data=validation_dataset.take(10),\n",
    "                            callbacks=[tensorboard_callback]\n",
    "                           )\n",
    "# Save the whole model\n",
    "path = \"training/model_full_{epoch:04d}\"\n",
    "model.save(path.format(epoch=nEpochs), save_format='tf')\n",
    "#Save model weights\n",
    "path = \"training/model_weights_{epoch:04d}.ckpt\"\n",
    "model.save_weights(path.format(epoch=nEpochs))\n",
    "\n",
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training end. Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The plots show the \"posterior\" probability that a given muons has $p_{T}$ in given bin. The correct behaviour is a peak round the true value, marked by a gren line.\n",
    "Unfortunetly a small fraction of muons with very low true $p_{T}$, around a few GeV/c receive a vey large $p_{T}$ assigned by the NN: this is a big bump around 100 GeV/c.\n",
    "This can be seen on the second plot: a efficency plot, where Y axis show fraction of muons with true $p_{T}$ marked by the X value, that get a NN or OMTF (human designed algorithm) greater or equal to 20 GeV/c. A small peak arount 3 GeV/c is visible. **Although this is small, it has very bad consequences, and has to be removed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = Jun 01 2021 09:47:58\n",
      "[[3.9308029e-06]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " ...\n",
      " [1.9510153e-08]\n",
      " [9.8261060e-03]\n",
      " [5.8680556e-08]]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-05d55c8e9ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             batch_df = pd.DataFrame(data={\"genPt\":labels[0], \"genCharge\":labels[1], \n\u001b[1;32m     29\u001b[0m                                           \u001b[0;34m\"OMTF_pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0momtfPredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OMTF_charge\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0momtfPredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                           \"NN_charge\":predictions[1]})\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#return df.append(batch_df, ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             )\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testIndex = 12\n",
    "\n",
    "if not runOnCluster:\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")   \n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    nEpochsSaved = 10\n",
    "    checkpoint_path = \"training/model_full_{epoch:04d}\"\n",
    "    model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "\n",
    "    testFileNames = glob.glob(testDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')    \n",
    "    test_dataset = loadDataset(testFileNames, isTrain=False, nEpochs=1, batchSize=32*1024)\n",
    "    \n",
    "    for aBatch in test_dataset.as_numpy_iterator():\n",
    "            labels = aBatch[1]\n",
    "            omtfPredictions = aBatch[2:4] \n",
    "            predictions = model.predict(aBatch[0], use_multiprocessing=True)\n",
    "            \n",
    "            ##TEST\n",
    "            features = aBatch[0]\n",
    "            labels = aBatch[1]\n",
    "            omtfPredictions = aBatch[2:4] \n",
    "            print(predictions[1])\n",
    "            predictions = model.predict(features, use_multiprocessing=True)\n",
    "            batch_df = pd.DataFrame(data={\"genPt\":labels[0], \"genCharge\":labels[1], \n",
    "                                          \"OMTF_pt\":omtfPredictions[0], \"OMTF_charge\":omtfPredictions[1], \n",
    "                                          \"NN_charge\":predictions[1]})\n",
    "            print(batch_df)\n",
    "            #return df.append(batch_df, ignore_index=True)\n",
    "            ##TEST\n",
    "            \n",
    "            #test = tf.reshape(predictions[0], (-1,43,1))\n",
    "            #y = tf.keras.layers.Conv1D(1, 5, activation='relu')(test)\n",
    "            #print(y.shape)\n",
    "            \n",
    "            plotPosterior(3, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(10, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(20, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(21, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(25, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(30, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(40, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPosterior(50, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPull(labels=labels[0], predictions=predictions[0], omtfPredictions=omtfPredictions[0])\n",
    "            plotCM(labels=labels, predictions=predictions, omtfPredictions=omtfPredictions)\n",
    "            break\n",
    "       \n",
    "    \n",
    "    #plotTurnOn(dataset = test_dataset, ptCut=5) \n",
    "    plotTurnOn(dataset = test_dataset, ptCut=10)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=20)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=22)\n",
    "    #plotTurnOn(dataset = test_dataset, ptCut=25)\n",
    "    #plotTurnOn(dataset = test_dataset, ptCut=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
