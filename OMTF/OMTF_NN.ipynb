{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMTF definitions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "workerEnv = str(os.getenv(\"USER\"))\n",
    "workOnPrometheus = workerEnv.find(\"plg\")>-1\n",
    "inputDataPrefix = \"/home/user1/scratch_ssd/akalinow/\"\n",
    "if workOnPrometheus:\n",
    "    inputDataPrefix = \"/net/people/plgakalinow/plggcmsml/\"\n",
    "    \n",
    "trainDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/OMTF/data/27_08_2020/\"   \n",
    "testDataDir = inputDataPrefix+\"/ProgrammingProjects/MachineLearning/OMTF/data/27_08_2020/\" \n",
    "\n",
    "\n",
    "nRefLayers = 8\n",
    "nLayers = 18\n",
    "nPDFBins = 2**7\n",
    "minProbability = 0.001\n",
    "minPlog = np.log(minProbability)\n",
    "nPdfValBits = 6\n",
    "refLayers = [0, 7, 2, 6, 16, 4, 10, 11]\n",
    "ptBins = np.concatenate((np.arange(0.0, 5, 0.5), np.arange(5, 30, 1), np.arange(30, 40, 5), np.arange(40, 60, 10), np.arange(60, 101, 20), [99999.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit --port=8008 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativePosteriorCut = 0.60\n",
    "testIndex = 0\n",
    "\n",
    "def plotPosterior(ptGen, labels, predictions):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    indices = np.logical_and(labels>ptGen-0.1, labels<ptGen+0.1)\n",
    "        \n",
    "    predictions = predictions[indices]\n",
    "    \n",
    "    ###TEST\n",
    "    '''\n",
    "    #predictions = predictions[:,0:45]\n",
    "    predictions = predictions[testIndex] \n",
    "    predictions = tf.reshape(predictions, (1,-1))\n",
    "    derivative = np.diff(predictions, append=0)\n",
    "    predictions = tf.math.divide_no_nan(derivative, predictions)\n",
    "    predictions = tf.abs(predictions)\n",
    "    #sum = tf.reduce_sum(predictions, axis=1, keepdims=True)\n",
    "    #predictions = 1.0/sum*predictions\n",
    "    '''\n",
    "    ######\n",
    "    \n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "    maxPosterior = tf.math.reduce_max(predictions)\n",
    "    scaleFactor = int(0.8/maxPosterior + 0.5)\n",
    "    axes[0].plot(label2Pt(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    "    axes[0].plot(label2Pt(np.arange(predictions.shape[0])), np.cumsum(predictions), linestyle='-.',label=\"cumulative posterior\")\n",
    "    axes[1].plot(label2Pt(np.arange(predictions.shape[0])), scaleFactor*predictions, label=\"{}xposterior\".format(scaleFactor))\n",
    "    \n",
    "    predictions = np.cumsum(predictions, axis=0)>cumulativePosteriorCut\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    ptRec = label2Pt(predictions)\n",
    "    print(\"Pt gen = {}, Pt rec {} cumulative posterior: {}\".format(ptGen, cumulativePosteriorCut, ptRec))\n",
    "    axes[0].axvline(ptGen, linestyle='-', color=\"olivedrab\", label=r'$p_{T}^{GEN} \\pm 1 [GeV/c]$')\n",
    "    axes[0].axvline(ptRec, linestyle='--', color=\"r\", label=r'$p_{T}^{REC} @ cum~post.=$'+str(cumulativePosteriorCut))\n",
    "    \n",
    "    axes[0].set_xlabel(r'$p_{T} [GeV/c]$')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].set_xlim([0, 2*ptGen])\n",
    "    axes[0].set_ylim([1E-3,1.05])\n",
    "    \n",
    "    axes[0].legend(bbox_to_anchor=(2.5,1), loc='upper left')\n",
    "    axes[1].set_xlabel(r'$p_{T} [GeV/c]$')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_xlim([0,201])\n",
    "    axes[1].set_ylim([1E-3,1.05])\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.3)\n",
    "    plt.savefig(\"fig_png/Posterior_ptGen_{}.png\".format(ptGen), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotTurnOn(dataset, ptCut):\n",
    "    ptMax = ptCut+50\n",
    "    #ptMax = 10 #TEST\n",
    "    nPtBins = int(ptMax*2.0)\n",
    "    ptHistoBins = range(0,nPtBins+1)\n",
    "\n",
    "    num = np.zeros(nPtBins)\n",
    "    numML = np.zeros(nPtBins)\n",
    "    denom = np.zeros(nPtBins)\n",
    "    \n",
    "    count =0\n",
    "    for aBatch in dataset.as_numpy_iterator():\n",
    "        labels = aBatch[1][0]\n",
    "        omtfPredictions = aBatch[2]\n",
    "        count += labels.shape[0]\n",
    "        predictions = model.predict(aBatch[0], use_multiprocessing=True)\n",
    "        predictions = predictions[0]\n",
    "        predictions = predictions[:,0:45] #TEST\n",
    "        predictions = np.cumsum(predictions, axis=1)>cumulativePosteriorCut\n",
    "        predictions = np.argmax(predictions, axis=1)   \n",
    "        predictions = label2Pt(predictions)\n",
    "        \n",
    "        tmp,_ = np.histogram(labels, bins=ptHistoBins)    \n",
    "        denom +=tmp\n",
    "        tmp,_ = np.histogram(labels[omtfPredictions>=ptCut], bins=ptHistoBins)\n",
    "        num += tmp\n",
    "        tmp,_ = np.histogram(labels[predictions>=ptCut], bins=ptHistoBins)\n",
    "        numML += tmp\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    ratio = np.divide(num, denom, out=np.zeros_like(denom), where=denom>0)\n",
    "    ratioML = np.divide(numML, denom, out=np.zeros_like(denom), where=denom>0)\n",
    "    axes[0].plot(ptHistoBins[:-1],num, label=\"OMTF\")\n",
    "    axes[0].plot(ptHistoBins[:-1],numML, label=\"ML\")\n",
    "    axes[0].set_xlim([0,2.0*ptCut])\n",
    "    #axes[0].set_ylim([0,1.0])\n",
    "    axes[0].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[0].set_ylabel('Events passing pT cut')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    \n",
    "    axes[1].plot(ptHistoBins[:-1],ratio, label=\"OMTF\")\n",
    "    axes[1].plot(ptHistoBins[:-1],ratioML, label=\"ML\")\n",
    "    axes[1].grid()\n",
    "    axes[1].set_yscale(\"log\")\n",
    "    axes[1].set_xlim([0,ptMax])\n",
    "    axes[1].set_ylim([1E-3,1.05])\n",
    "    axes[1].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[1].set_ylabel('Efficiency')\n",
    "\n",
    "    axes[2].plot(ptHistoBins[:-1],ratio, label=\"OMTF\")\n",
    "    axes[2].plot(ptHistoBins[:-1],ratioML, label=\"ML\")\n",
    "    axes[2].grid()\n",
    "    axes[2].axhline(y=0.5)\n",
    "    axes[2].axhline(y=0.85)\n",
    "    axes[2].axvline(x=ptCut)\n",
    "    axes[2].set_xlim([0,ptMax])\n",
    "    axes[2].set_ylim([0.0,1.05])\n",
    "    axes[2].set_xlabel(r'$p_{T}^{GEN}$')\n",
    "    axes[2].set_ylabel('Efficiency')\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.5)\n",
    "    plt.savefig(\"fig_png/TurnOn_ptCut_{}.png\".format(ptCut), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plotPull(labels, predictions, omtfPredictions):\n",
    "    \n",
    "    minX = -1\n",
    "    maxX = 2\n",
    "    nBins = 50\n",
    "    predictions = np.cumsum(predictions, axis=1)>cumulativePosteriorCut\n",
    "    predictions = np.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)   \n",
    "    error = (predictions - labels)/labels\n",
    "    omtfError = (omtfPredictions - labels)/labels    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 5))  \n",
    "    axes[0].hist(error, range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[0].hist(omtfError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"OMTF\")\n",
    "    axes[0].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].set_xlim([minX, maxX])\n",
    "    #axes[0].set_ylim([-2,2])\n",
    "    \n",
    "    axes[1].hist(omtfError, range=(minX, maxX), bins = nBins, color=\"tomato\", label=\"OMTF\")\n",
    "    axes[1].hist(error, range=(minX, maxX), bins = nBins, color=\"deepskyblue\", label = \"NN\")\n",
    "    axes[1].set_xlabel(\"(Model - True)/True\")\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_xlim([minX, maxX])\n",
    "    plt.savefig(\"fig_png/Pull.png\", bbox_inches=\"tight\")\n",
    " \n",
    "def plotCM(labels, predictions, omtfPredictions):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize = (10, 10))  \n",
    "    \n",
    "    ptMax =  ptBins.shape[0]  \n",
    "    vmax = 1.0\n",
    "    ptPredictions = np.cumsum(predictions[0], axis=1)>cumulativePosteriorCut\n",
    "    ptPredictions = np.argmax(ptPredictions, axis=1)   \n",
    "    cm = tf.math.confusion_matrix(pT2Label(labels[0]), ptPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    #vmax = tf.math.reduce_max(cm)\n",
    "    \n",
    "    myPalette = sns.color_palette(\"YlGnBu\", n_colors=20)\n",
    "    myPalette[0] = (1,1,1)\n",
    "    \n",
    "    vmax = 0.1 #TEST\n",
    "    sns.heatmap(cm, ax = axes[0,0], vmax = vmax, annot=False, xticklabels=4, yticklabels=4, cmap=myPalette)\n",
    "    axes[0,0].set_ylabel(r'$p_{T}^{NN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].set_xlabel(r'$p_{T}^{GEN} \\rm{[bin ~number]}$');\n",
    "    axes[0,0].grid()\n",
    "    axes[0,0].set_ylim([0,ptMax])\n",
    "    axes[0,0].set_xlim([0,ptMax])\n",
    "    axes[0,0].set_aspect(aspect='equal')\n",
    "    axes[0,0].set_title(\"NN\")\n",
    "    \n",
    "    cm = tf.math.confusion_matrix(pT2Label(labels[0]), pT2Label(omtfPredictions[0]))\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    #vmax = tf.math.reduce_max(cm)\n",
    "    sns.heatmap(cm, ax = axes[0,1], vmax = vmax, annot=False, xticklabels=4, yticklabels=4, cmap=myPalette)\n",
    "    axes[0,1].grid()\n",
    "    axes[0,1].set_title(\"OMTF\")\n",
    "    axes[0,1].set_xlim([0,ptMax])\n",
    "    axes[0,1].set_ylim([0,ptMax])\n",
    "    axes[0,1].set_aspect(aspect='equal')\n",
    "    axes[0,1].set_ylabel(r'$p_{T}^{OMTF} \\rm{[bin ~number]}$')\n",
    "    axes[0,1].set_xlabel(r'$p_{T}^{GEN} \\rm{[bin ~number]}$') \n",
    "        \n",
    "    chPredictions =tf.map_fn(lambda x: x>0.5, predictions[1], dtype=tf.bool)\n",
    "    chPredictions = tf.reshape(chPredictions, (-1,1))\n",
    "    chLabels =tf.map_fn(lambda x: x>0.5, labels[1], dtype=tf.bool)\n",
    "    #chLabels = tf.reshape(chLabels, (-1,1))\n",
    "    \n",
    "    vmax = 1.0\n",
    "    vmin = 0.0\n",
    "    cm = tf.math.confusion_matrix(chLabels, chPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    sns.heatmap(cm, ax = axes[1,0], vmax = vmax, annot=True, cmap=myPalette)\n",
    "    axes[1,0].set_title(\"NN\")\n",
    "    axes[1,0].set_aspect(aspect='equal')\n",
    "    axes[1,0].set_ylabel(r'$q^{NN}$')\n",
    "    axes[1,0].set_xlabel(r'$q^{GEN}$') \n",
    "    \n",
    "    chPredictions =tf.map_fn(lambda x: x>0.5, omtfPredictions[1], dtype=tf.bool)  \n",
    "    chPredictions = tf.reshape(chPredictions, (-1,1))\n",
    "    cm = tf.math.confusion_matrix(chLabels, chPredictions)\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = tf.math.divide_no_nan(cm, tf.math.reduce_sum(cm, axis=1)[:, np.newaxis])\n",
    "    cm = tf.transpose(cm)\n",
    "    sns.heatmap(cm, ax = axes[1,1], vmax = vmax, annot=True, cmap=myPalette, linewidths=0.01)\n",
    "    axes[1,1].set_title(\"OMTF\")\n",
    "    axes[1,1].set_aspect(aspect='equal')\n",
    "    axes[1,1].set_ylabel(r'$q^{NN}$')\n",
    "    axes[1,1].set_xlabel(r'$q^{GEN}$') \n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.25, hspace=0.35)\n",
    "    plt.savefig(\"fig_png/CM.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(['muonPt', 'muonEta', 'muonPhi', 'muonCharge', 'omtfPt', 'omtfEta',\n",
    "       'omtfPhi', 'omtfCharge', 'omtfScore', 'omtfQuality', 'omtfRefLayer',\n",
    "       'omtfProcessor', 'omtfFiredLayers', 'phiDist_0', 'phiDist_1',\n",
    "       'phiDist_2', 'phiDist_3', 'phiDist_4', 'phiDist_5', 'phiDist_6',\n",
    "       'phiDist_7', 'phiDist_8', 'phiDist_9', 'phiDist_10', 'phiDist_11',\n",
    "       'phiDist_12', 'phiDist_13', 'phiDist_14', 'phiDist_15', 'phiDist_16',\n",
    "       'phiDist_17'])\n",
    "\n",
    "def getFeaturesMask():\n",
    "    featuresMask = np.full_like(columns, False, dtype=np.bool)\n",
    "    for iLayer in range(0, 18):\n",
    "        featureLabel = \"phiDist_{}\".format(iLayer)\n",
    "        featuresMask += (columns==featureLabel)\n",
    "    #featuresMask += columns==\"omtfFiredLayers\"\n",
    "    #featuresMask += columns==\"omtfRefLayer\"\n",
    "    #featuresMask += columns==\"omtfPt\"\n",
    "    #featuresMask = columns==\"omtfQuality\"\n",
    "    return featuresMask\n",
    "\n",
    "def getFeature(name, dataRow):\n",
    "    columnIndex = np.where(columns == name)[0][0]  \n",
    "    return dataRow[:,columnIndex]\n",
    "\n",
    "def parse_tensor(tensor):\n",
    "    return tf.io.parse_tensor(tensor, out_type=tf.float64)\n",
    "\n",
    "def pT2Label(tensor):\n",
    "    tensor = tf.searchsorted(ptBins, tensor, side='left')\n",
    "    return tensor\n",
    "    \n",
    "def label2Pt(tensor):  \n",
    "    return tf.where(ptBins[tensor]<9999, ptBins[tensor], [200])\n",
    "\n",
    "def modifyFeatures(dataRow, batchSize, isTrain=False):\n",
    "    columnsMask = getFeaturesMask()\n",
    "    features = tf.boolean_mask(dataRow, columnsMask, axis=1)\n",
    "    dummyValue = 0\n",
    "    features = tf.where(features<9999, features, dummyValue) \n",
    "    features.set_shape([batchSize,np.count_nonzero(columnsMask)])\n",
    "    \n",
    "    columnIndex = np.where(columns == \"muonCharge\")[0][0]  \n",
    "    chargeLabels = (dataRow[:,columnIndex]+1)/2 \n",
    "    chargeLabels.set_shape([batchSize,])\n",
    "    \n",
    "    columnIndex = np.where(columns == \"muonPt\")[0][0]\n",
    "    ptLabels = dataRow[:,columnIndex]\n",
    "    ptLabels.set_shape([batchSize,])\n",
    "    trainWeight = 1.0#tf.math.exp(-ptLabels/10.0)\n",
    "    \n",
    "    if isTrain:\n",
    "        ptLabels = pT2Label(ptLabels)\n",
    "        return (features, (ptLabels, chargeLabels), trainWeight)\n",
    "    else:\n",
    "        columnIndex = np.where(columns == \"omtfPt\")[0][0]  \n",
    "        omtfPt = dataRow[:,columnIndex]\n",
    "        columnIndex = np.where(columns == \"omtfQuality\")[0][0]  \n",
    "        omtfQuality = dataRow[:,columnIndex]\n",
    "        omtfPt = tf.where(omtfQuality>=12, omtfPt, 0) \n",
    "        omtfPt.set_shape([batchSize,])\n",
    "        \n",
    "        columnIndex = np.where(columns == \"omtfCharge\")[0][0]  \n",
    "        omtfCharge = dataRow[:,columnIndex]\n",
    "        omtfCharge.set_shape([batchSize,])\n",
    "        return (features, (ptLabels, chargeLabels), omtfPt, omtfCharge)\n",
    "    return dataRow\n",
    "    \n",
    "def loadDataset(fileNames, isTrain, nEpochs=1, batchSize=1):   \n",
    "    raw_dataset = tf.data.TFRecordDataset(fileNames, compression_type=\"GZIP\") #GZIP\n",
    "    dataset = raw_dataset.map(parse_tensor,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batchSize, drop_remainder=True)\n",
    "    #Split data into [features, labels] and modify features\n",
    "    dataset = dataset.map(lambda x: modifyFeatures(x, batchSize, isTrain))\n",
    "    return dataset\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(1E-10)\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_mean_metric(y_true, y_pred):    \n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Pt(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels   \n",
    "    mean = tf.math.reduce_mean(pull, axis=0)\n",
    "    return mean \n",
    "    \n",
    "def pull_variance_metric(y_true, y_pred):\n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)   \n",
    "    predictions = label2Pt(predictions)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    labels = label2Pt(tf.cast(y_true, tf.int32))\n",
    "    pull = (labels - predictions)/labels  \n",
    "    variance = tf.math.reduce_variance(pull, axis=0) \n",
    "    return tf.sqrt(variance) \n",
    "\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    \n",
    "    predictions = tf.math.cumsum(y_pred, axis=1)>cumulativePosteriorCut\n",
    "    predictions = tf.math.argmax(predictions, axis=1)\n",
    "    predictions = tf.reshape(predictions, (-1,1))\n",
    "    #lowPtLoss = (labels<(10))*(predictions>(15))\n",
    "    labels = y_true\n",
    "\n",
    "    lowPtLoss = tf.math.logical_and(tf.math.less(labels, 10), tf.math.greater(predictions, 10))\n",
    "    lowPtLoss = (predictions-labels)*lowPtLoss\n",
    "    lowPtLoss = tf.cast(lowPtLoss, float32)\n",
    "    \n",
    "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true, y_pred) + lowPtLoss\n",
    "\n",
    "\n",
    "custom_objects={'pull_mean_metric': pull_mean_metric,\n",
    "                'pull_variance_metric':pull_variance_metric,\n",
    "                'my_loss_fn':my_loss_fn\n",
    "               }\n",
    "def getModel():\n",
    "    \n",
    "  nPtBins =  ptBins.shape[0]\n",
    "  nInputs = np.sum(getFeaturesMask())\n",
    "  inputs = tf.keras.Input(shape=(nInputs,), name=\"deltaPhi\")\n",
    "  activation = tf.keras.layers.Activation(tf.nn.leaky_relu)\n",
    "    \n",
    "  ptLayer = inputs\n",
    "  for iLayer in range(0,10):\n",
    "            ptLayer = tf.keras.layers.Dense(256, activation=activation, name=\"Pt_layer_{}\".format(iLayer))(ptLayer)\n",
    "    \n",
    "  chargeLayer= inputs \n",
    "  for iLayer in range(0,1): \n",
    "       chargeLayer = tf.keras.layers.Dense(32, activation=activation, name=\"Charge_layer_{}\".format(iLayer))(chargeLayer)\n",
    "    \n",
    "  ptOutput = tf.keras.layers.Dense(nPtBins, activation=tf.nn.softmax,bias_initializer='zeros',name = \"pt\")(ptLayer) \n",
    "  chargeOutput = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid,name = \"charge\")(chargeLayer)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=[ptOutput, chargeOutput], name=\"NN_OMTF\")\n",
    "\n",
    "  initial_learning_rate = 0.005\n",
    "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True)\n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss={\"pt\":tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      \"charge\":tf.keras.losses.BinaryCrossentropy(from_logits=False)},\n",
    "                metrics=['accuracy'])\n",
    "  tf.keras.utils.plot_model(model, 'fig_png/ML_model.png', show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=True, nEpochs=1, batchSize=2)\n",
    "\n",
    "#dataset = train_dataset.take(10000)\n",
    "#benchmark(dataset)\n",
    "#benchmark(dataset.prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "mask = tf.constant([0,1,2,3,10,11,12,13], dtype=tf.int32)\n",
    "print(mask)\n",
    "\n",
    "for element in train_dataset.take(10): \n",
    "  print(element[0])\n",
    "  y = tf.gather(params=element[0], indices=mask, axis=1)\n",
    "  print(y)  \n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training start. Current Time =\", current_time)\n",
    "\n",
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_oldSample_files_15_25_chunk_0.tfrecord.gzip')\n",
    "validationFileNames = glob.glob(testDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')\n",
    "\n",
    "train_dataset = loadDataset(trainFileNames, isTrain=True, nEpochs=1, batchSize=2*4096)\n",
    "validation_dataset = loadDataset(validationFileNames, isTrain=True, nEpochs=1, batchSize=1024)\n",
    "\n",
    "model = getModel()\n",
    "\n",
    "#nEpochsSaved = 5\n",
    "#checkpoint_path = \"training/model_full_{epoch:04d}\"\n",
    "#model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "nEpochs = 5\n",
    "history = model.fit(train_dataset, epochs=nEpochs,\n",
    "                    use_multiprocessing=True,\n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    validation_data=validation_dataset.take(10),\n",
    "                    callbacks=[tensorboard_callback]\n",
    "                   )\n",
    "\n",
    "# Save the whole model\n",
    "path = \"training/model_full_{epoch:04d}\"\n",
    "model.save(path.format(epoch=nEpochs), save_format='tf')\n",
    "#Save model weights\n",
    "path = \"training/model_weights_{epoch:04d}.ckpt\"\n",
    "model.save_weights(path.format(epoch=nEpochs))\n",
    "\n",
    "current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "print(\"Training end. Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = Dec 15 2020 08:30:42\n",
      "Pt gen = 3, Pt rec 0.6 cumulative posterior: [4.]\n",
      "Pt gen = 10, Pt rec 0.6 cumulative posterior: [11.]\n",
      "Pt gen = 20, Pt rec 0.6 cumulative posterior: [21.]\n",
      "Pt gen = 21, Pt rec 0.6 cumulative posterior: [21.]\n",
      "Pt gen = 22, Pt rec 0.6 cumulative posterior: [23.]\n",
      "Pt gen = 30, Pt rec 0.6 cumulative posterior: [30.]\n",
      "Pt gen = 40, Pt rec 0.6 cumulative posterior: [50.]\n",
      "Pt gen = 50, Pt rec 0.6 cumulative posterior: [60.]\n"
     ]
    }
   ],
   "source": [
    "testIndex = 11\n",
    "\n",
    "if not workOnPrometheus:\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")   \n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    nEpochsSaved = 250\n",
    "    checkpoint_path = \"training/model_full_{epoch:04d}\"\n",
    "    model = tf.keras.models.load_model(checkpoint_path.format(epoch=nEpochsSaved), custom_objects=custom_objects)\n",
    "\n",
    "    testFileNames = glob.glob(testDataDir+'OMTFHits_pats0x0003_newerSample_files_1_100_chunk_0.tfrecord.gzip')    \n",
    "    test_dataset = loadDataset(testFileNames, isTrain=False, nEpochs=1, batchSize=32*1024)\n",
    "    \n",
    "    for aBatch in test_dataset.as_numpy_iterator():\n",
    "            labels = aBatch[1]\n",
    "            omtfPredictions = aBatch[2:4] \n",
    "            predictions = model.predict(aBatch[0], use_multiprocessing=True)         \n",
    "            plotPosterior(3, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(10, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(20, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(21, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(22, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(30, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(40, labels=labels[0], predictions=predictions[0])\n",
    "            plotPosterior(50, labels=labels[0], predictions=predictions[0])\n",
    "            #plotPull(labels=labels[0], predictions=predictions[0], omtfPredictions=omtfPredictions[0])\n",
    "            #plotCM(labels=labels, predictions=predictions, omtfPredictions=omtfPredictions)\n",
    "            break\n",
    "       \n",
    "    \n",
    "    plotTurnOn(dataset = test_dataset, ptCut=5) \n",
    "    plotTurnOn(dataset = test_dataset, ptCut=10)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=20)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=22)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=25)\n",
    "    plotTurnOn(dataset = test_dataset, ptCut=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
